{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Homework_04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network Homework\n",
        "\n",
        "This is the 4th assignment for CAP 4630 and we will implement a basic RNN network and an LSTM network with Keras to solve two problems. \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 points, including 15 bonus points)** \\\n",
        "You may use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data (17 Points)\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the given train and test data: \"train.txt\" and \"test.txt\". **(5 Points)**\n",
        "2. Generate the **TRAIN** and **TEST** labels. **(5 Points)**\n",
        "3. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". **(5 Points)**\n",
        "4. **PRINT OUT** the **TEST** data and label. **(2 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. The length of original train data is 113 which starts from **\"1949-01\"** to **\"1958-05\"**. The length of original test data is 29, which starts from **\"1958-07\"** to **\"1960-11\"**. \n",
        "2. Set the data types of both train and test data to \"float32\". \n",
        "3. When you prepared input data X (sequences) and oupt data Y (labels), please consider the following relationship:\n",
        "    - The sequence X should be the **past 12** datapoints in the time series, i.e., observation sequence with historical window of 12. You may check the time series data and think about the reason.\n",
        "    - The label Y should be the **next 1** datapoint in the time series (one point ahead prediction).\n",
        "4. The first 3 **TRAIN** data and label should be:\n",
        "\n",
        "- trainX[0] = [[0.02203858 &nbsp; 0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp; 0.03856748]]\n",
        "- trainY[0] = [0.03030303]\n",
        "\n",
        "- trianX[1] = [[0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197  &nbsp; 0.08539945  &nbsp; 0.12121212  &nbsp; 0.12121212  &nbsp; 0.08815429  &nbsp; 0.04132232  &nbsp; 0.     &nbsp;  0.03856748   &nbsp; 0.03030303]]\n",
        "- trainY[1] = [0.06060606]\n",
        "\n",
        "- trainX[2] =  [[0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp;     0.03856748 &nbsp; 0.03030303 &nbsp; 0.06060606]]\n",
        "- trainY[2] = [0.10192838]\n",
        "\n",
        "5. Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "\n",
        "6. After the preparation with scaler fitting, the shapes of trainX, trainY, testX, and testY are as follows:\\\n",
        "trainX.shape = (101, 1, 12)\\\n",
        "trainY.shape = (101,)\\\n",
        "testX.shape = (17, 1, 12)\\\n",
        "testY.shape = (17,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "source": [
        "### Import Libraries ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "reset_random_seeds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAj2Y6IuxUv",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "66eeea15-ec61-4b7b-c883-3af11ca1751b"
      },
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "from pandas import read_csv\n",
        "from google.colab import files\n",
        "trainFile = files.upload()\n",
        "testFile = files.upload()\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "\n",
        "def create_dataset(dataset, look_back=12): # look_back is used to specify input sequence length\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back):\n",
        "\t\tdataX.append(dataset[i:(i + look_back), 0]) \n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "dataframe_train = read_csv('train.txt', usecols=[1], engine='python') # Read train.txt \n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "dataframe_test = read_csv('test.txt', usecols=[1], engine='python') # Read test.txt \n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "train = scaler.fit_transform(dataset_train) # fit the scaler to the training data\n",
        "test = scaler.transform(dataset_test) # fit the scaler to the test data\n",
        "\n",
        "### Training and Test Data Split ###\n",
        "trainX, trainY = create_dataset(train, look_back=12)\n",
        "testX, testY = create_dataset(test, look_back=12)\n",
        "\n",
        "### Training and Test Data Reshape (to fit RNN input) ###\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2d801f5-377a-4618-94bd-ec54dbc818c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2d801f5-377a-4618-94bd-ec54dbc818c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.txt to train.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52b1c995-a43b-44d2-b51c-ad0f47a7b44f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52b1c995-a43b-44d2-b51c-ad0f47a7b44f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.txt to test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93796132-c03e-471d-8691-d583e3e068a9"
      },
      "source": [
        "### Print Out the TEST Data and Labels Here ###\n",
        "print(\"trainX[0]= \", trainX[0], \"\\n\", \"trainY[0]= \", trainY[0], \"\\n\", \"trainX[1]= \", trainX[1], \"\\n\", \"trainY[1]= \", trainY[1], \"\\n\", \"trainX[2]= \", trainX[2], \"\\n\", \"trainY[2]= \", trainY[2], \"\\n\", sep='')\n",
        "print(\"trainX.shape= \", trainX.shape, \"\\n\", \"trainY.shape= \", trainY.shape, \"\\n\", \"testX.shape= \", testX.shape, \"\\n\", \"testY.shape= \", testY.shape, \"\\n\", sep='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX[0]= [[0.02203858 0.03856748 0.077135   0.06887051 0.04683197 0.08539945\n",
            "  0.12121212 0.12121212 0.08815429 0.04132232 0.         0.03856748]]\n",
            "trainY[0]= 0.030303031\n",
            "trainX[1]= [[0.03856748 0.077135   0.06887051 0.04683197 0.08539945 0.12121212\n",
            "  0.12121212 0.08815429 0.04132232 0.         0.03856748 0.03030303]]\n",
            "trainY[1]= 0.060606062\n",
            "trainX[2]= [[0.077135   0.06887051 0.04683197 0.08539945 0.12121212 0.12121212\n",
            "  0.08815429 0.04132232 0.         0.03856748 0.03030303 0.06060606]]\n",
            "trainY[2]= 0.10192838\n",
            "\n",
            "trainX.shape= (101, 1, 12)\n",
            "trainY.shape= (101,)\n",
            "testX.shape= (17, 1, 12)\n",
            "testY.shape= (17,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model (20 Points) ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 simple RNN layer and 1 Dense layer.  **(10 Points)**\n",
        "2. Compile the model. **(5 Points)**\n",
        "3. Train the model for **1000** epochs with **batch_size = 10**. **(5 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may consider **tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "2. Use loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.005** and other default settings.\n",
        "3. After first epoch, the train loss is changed to around **0.0912**. \n",
        "4. The model summary is as follows:\n",
        "- Total params: 73\n",
        "- Trainable params: 73\n",
        "- Non-trainable params: 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(keras.layers.SimpleRNN(4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(1)) # Add a following Dense layer with units=1 in the model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "source": [
        "### Compile the RNN Model  ###\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error') # model compiled with mean_squared_error loss and adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9726711a-6256-4a50-dd40-b5c7b942d585"
      },
      "source": [
        "### Train the RNN Model  ###\n",
        "\n",
        "model.fit(trainX, trainY, epochs=1000, batch_size=10, verbose=2, validation_data=(testX, testY)) # model fit with epoch=1000, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "11/11 - 4s - loss: 0.1754 - val_loss: 0.8825 - 4s/epoch - 338ms/step\n",
            "Epoch 2/1000\n",
            "11/11 - 0s - loss: 0.1121 - val_loss: 0.6535 - 69ms/epoch - 6ms/step\n",
            "Epoch 3/1000\n",
            "11/11 - 0s - loss: 0.0695 - val_loss: 0.4797 - 70ms/epoch - 6ms/step\n",
            "Epoch 4/1000\n",
            "11/11 - 0s - loss: 0.0433 - val_loss: 0.3687 - 78ms/epoch - 7ms/step\n",
            "Epoch 5/1000\n",
            "11/11 - 0s - loss: 0.0317 - val_loss: 0.2961 - 79ms/epoch - 7ms/step\n",
            "Epoch 6/1000\n",
            "11/11 - 0s - loss: 0.0268 - val_loss: 0.2526 - 73ms/epoch - 7ms/step\n",
            "Epoch 7/1000\n",
            "11/11 - 0s - loss: 0.0246 - val_loss: 0.2348 - 87ms/epoch - 8ms/step\n",
            "Epoch 8/1000\n",
            "11/11 - 0s - loss: 0.0240 - val_loss: 0.2272 - 77ms/epoch - 7ms/step\n",
            "Epoch 9/1000\n",
            "11/11 - 0s - loss: 0.0235 - val_loss: 0.2219 - 81ms/epoch - 7ms/step\n",
            "Epoch 10/1000\n",
            "11/11 - 0s - loss: 0.0230 - val_loss: 0.2161 - 82ms/epoch - 7ms/step\n",
            "Epoch 11/1000\n",
            "11/11 - 0s - loss: 0.0226 - val_loss: 0.2100 - 79ms/epoch - 7ms/step\n",
            "Epoch 12/1000\n",
            "11/11 - 0s - loss: 0.0218 - val_loss: 0.1929 - 85ms/epoch - 8ms/step\n",
            "Epoch 13/1000\n",
            "11/11 - 0s - loss: 0.0216 - val_loss: 0.1907 - 71ms/epoch - 6ms/step\n",
            "Epoch 14/1000\n",
            "11/11 - 0s - loss: 0.0212 - val_loss: 0.1806 - 92ms/epoch - 8ms/step\n",
            "Epoch 15/1000\n",
            "11/11 - 0s - loss: 0.0206 - val_loss: 0.1807 - 76ms/epoch - 7ms/step\n",
            "Epoch 16/1000\n",
            "11/11 - 0s - loss: 0.0201 - val_loss: 0.1871 - 78ms/epoch - 7ms/step\n",
            "Epoch 17/1000\n",
            "11/11 - 0s - loss: 0.0197 - val_loss: 0.1875 - 78ms/epoch - 7ms/step\n",
            "Epoch 18/1000\n",
            "11/11 - 0s - loss: 0.0194 - val_loss: 0.1857 - 81ms/epoch - 7ms/step\n",
            "Epoch 19/1000\n",
            "11/11 - 0s - loss: 0.0191 - val_loss: 0.1856 - 85ms/epoch - 8ms/step\n",
            "Epoch 20/1000\n",
            "11/11 - 0s - loss: 0.0187 - val_loss: 0.1811 - 74ms/epoch - 7ms/step\n",
            "Epoch 21/1000\n",
            "11/11 - 0s - loss: 0.0183 - val_loss: 0.1717 - 77ms/epoch - 7ms/step\n",
            "Epoch 22/1000\n",
            "11/11 - 0s - loss: 0.0178 - val_loss: 0.1621 - 83ms/epoch - 8ms/step\n",
            "Epoch 23/1000\n",
            "11/11 - 0s - loss: 0.0174 - val_loss: 0.1477 - 89ms/epoch - 8ms/step\n",
            "Epoch 24/1000\n",
            "11/11 - 0s - loss: 0.0171 - val_loss: 0.1473 - 76ms/epoch - 7ms/step\n",
            "Epoch 25/1000\n",
            "11/11 - 0s - loss: 0.0167 - val_loss: 0.1446 - 73ms/epoch - 7ms/step\n",
            "Epoch 26/1000\n",
            "11/11 - 0s - loss: 0.0163 - val_loss: 0.1449 - 73ms/epoch - 7ms/step\n",
            "Epoch 27/1000\n",
            "11/11 - 0s - loss: 0.0159 - val_loss: 0.1371 - 83ms/epoch - 8ms/step\n",
            "Epoch 28/1000\n",
            "11/11 - 0s - loss: 0.0157 - val_loss: 0.1280 - 93ms/epoch - 8ms/step\n",
            "Epoch 29/1000\n",
            "11/11 - 0s - loss: 0.0153 - val_loss: 0.1292 - 73ms/epoch - 7ms/step\n",
            "Epoch 30/1000\n",
            "11/11 - 0s - loss: 0.0148 - val_loss: 0.1301 - 75ms/epoch - 7ms/step\n",
            "Epoch 31/1000\n",
            "11/11 - 0s - loss: 0.0146 - val_loss: 0.1344 - 76ms/epoch - 7ms/step\n",
            "Epoch 32/1000\n",
            "11/11 - 0s - loss: 0.0143 - val_loss: 0.1301 - 74ms/epoch - 7ms/step\n",
            "Epoch 33/1000\n",
            "11/11 - 0s - loss: 0.0139 - val_loss: 0.1275 - 75ms/epoch - 7ms/step\n",
            "Epoch 34/1000\n",
            "11/11 - 0s - loss: 0.0137 - val_loss: 0.1260 - 76ms/epoch - 7ms/step\n",
            "Epoch 35/1000\n",
            "11/11 - 0s - loss: 0.0133 - val_loss: 0.1213 - 77ms/epoch - 7ms/step\n",
            "Epoch 36/1000\n",
            "11/11 - 0s - loss: 0.0130 - val_loss: 0.1163 - 83ms/epoch - 8ms/step\n",
            "Epoch 37/1000\n",
            "11/11 - 0s - loss: 0.0128 - val_loss: 0.1143 - 78ms/epoch - 7ms/step\n",
            "Epoch 38/1000\n",
            "11/11 - 0s - loss: 0.0125 - val_loss: 0.1052 - 74ms/epoch - 7ms/step\n",
            "Epoch 39/1000\n",
            "11/11 - 0s - loss: 0.0125 - val_loss: 0.1025 - 72ms/epoch - 7ms/step\n",
            "Epoch 40/1000\n",
            "11/11 - 0s - loss: 0.0121 - val_loss: 0.1064 - 81ms/epoch - 7ms/step\n",
            "Epoch 41/1000\n",
            "11/11 - 0s - loss: 0.0116 - val_loss: 0.1057 - 74ms/epoch - 7ms/step\n",
            "Epoch 42/1000\n",
            "11/11 - 0s - loss: 0.0114 - val_loss: 0.1011 - 72ms/epoch - 7ms/step\n",
            "Epoch 43/1000\n",
            "11/11 - 0s - loss: 0.0111 - val_loss: 0.1016 - 78ms/epoch - 7ms/step\n",
            "Epoch 44/1000\n",
            "11/11 - 0s - loss: 0.0109 - val_loss: 0.1018 - 86ms/epoch - 8ms/step\n",
            "Epoch 45/1000\n",
            "11/11 - 0s - loss: 0.0108 - val_loss: 0.1003 - 79ms/epoch - 7ms/step\n",
            "Epoch 46/1000\n",
            "11/11 - 0s - loss: 0.0106 - val_loss: 0.0941 - 75ms/epoch - 7ms/step\n",
            "Epoch 47/1000\n",
            "11/11 - 0s - loss: 0.0102 - val_loss: 0.0963 - 74ms/epoch - 7ms/step\n",
            "Epoch 48/1000\n",
            "11/11 - 0s - loss: 0.0100 - val_loss: 0.0939 - 79ms/epoch - 7ms/step\n",
            "Epoch 49/1000\n",
            "11/11 - 0s - loss: 0.0098 - val_loss: 0.0884 - 72ms/epoch - 7ms/step\n",
            "Epoch 50/1000\n",
            "11/11 - 0s - loss: 0.0095 - val_loss: 0.0802 - 76ms/epoch - 7ms/step\n",
            "Epoch 51/1000\n",
            "11/11 - 0s - loss: 0.0095 - val_loss: 0.0789 - 77ms/epoch - 7ms/step\n",
            "Epoch 52/1000\n",
            "11/11 - 0s - loss: 0.0091 - val_loss: 0.0826 - 71ms/epoch - 6ms/step\n",
            "Epoch 53/1000\n",
            "11/11 - 0s - loss: 0.0089 - val_loss: 0.0825 - 87ms/epoch - 8ms/step\n",
            "Epoch 54/1000\n",
            "11/11 - 0s - loss: 0.0087 - val_loss: 0.0809 - 84ms/epoch - 8ms/step\n",
            "Epoch 55/1000\n",
            "11/11 - 0s - loss: 0.0086 - val_loss: 0.0769 - 83ms/epoch - 8ms/step\n",
            "Epoch 56/1000\n",
            "11/11 - 0s - loss: 0.0083 - val_loss: 0.0769 - 75ms/epoch - 7ms/step\n",
            "Epoch 57/1000\n",
            "11/11 - 0s - loss: 0.0081 - val_loss: 0.0732 - 72ms/epoch - 7ms/step\n",
            "Epoch 58/1000\n",
            "11/11 - 0s - loss: 0.0079 - val_loss: 0.0751 - 81ms/epoch - 7ms/step\n",
            "Epoch 59/1000\n",
            "11/11 - 0s - loss: 0.0078 - val_loss: 0.0730 - 74ms/epoch - 7ms/step\n",
            "Epoch 60/1000\n",
            "11/11 - 0s - loss: 0.0076 - val_loss: 0.0752 - 81ms/epoch - 7ms/step\n",
            "Epoch 61/1000\n",
            "11/11 - 0s - loss: 0.0075 - val_loss: 0.0716 - 72ms/epoch - 7ms/step\n",
            "Epoch 62/1000\n",
            "11/11 - 0s - loss: 0.0073 - val_loss: 0.0709 - 85ms/epoch - 8ms/step\n",
            "Epoch 63/1000\n",
            "11/11 - 0s - loss: 0.0071 - val_loss: 0.0690 - 79ms/epoch - 7ms/step\n",
            "Epoch 64/1000\n",
            "11/11 - 0s - loss: 0.0068 - val_loss: 0.0631 - 71ms/epoch - 6ms/step\n",
            "Epoch 65/1000\n",
            "11/11 - 0s - loss: 0.0068 - val_loss: 0.0641 - 78ms/epoch - 7ms/step\n",
            "Epoch 66/1000\n",
            "11/11 - 0s - loss: 0.0066 - val_loss: 0.0594 - 76ms/epoch - 7ms/step\n",
            "Epoch 67/1000\n",
            "11/11 - 0s - loss: 0.0065 - val_loss: 0.0599 - 73ms/epoch - 7ms/step\n",
            "Epoch 68/1000\n",
            "11/11 - 0s - loss: 0.0063 - val_loss: 0.0634 - 76ms/epoch - 7ms/step\n",
            "Epoch 69/1000\n",
            "11/11 - 0s - loss: 0.0061 - val_loss: 0.0553 - 73ms/epoch - 7ms/step\n",
            "Epoch 70/1000\n",
            "11/11 - 0s - loss: 0.0060 - val_loss: 0.0539 - 85ms/epoch - 8ms/step\n",
            "Epoch 71/1000\n",
            "11/11 - 0s - loss: 0.0058 - val_loss: 0.0563 - 79ms/epoch - 7ms/step\n",
            "Epoch 72/1000\n",
            "11/11 - 0s - loss: 0.0057 - val_loss: 0.0566 - 78ms/epoch - 7ms/step\n",
            "Epoch 73/1000\n",
            "11/11 - 0s - loss: 0.0056 - val_loss: 0.0564 - 79ms/epoch - 7ms/step\n",
            "Epoch 74/1000\n",
            "11/11 - 0s - loss: 0.0056 - val_loss: 0.0504 - 80ms/epoch - 7ms/step\n",
            "Epoch 75/1000\n",
            "11/11 - 0s - loss: 0.0054 - val_loss: 0.0523 - 78ms/epoch - 7ms/step\n",
            "Epoch 76/1000\n",
            "11/11 - 0s - loss: 0.0052 - val_loss: 0.0526 - 78ms/epoch - 7ms/step\n",
            "Epoch 77/1000\n",
            "11/11 - 0s - loss: 0.0052 - val_loss: 0.0493 - 70ms/epoch - 6ms/step\n",
            "Epoch 78/1000\n",
            "11/11 - 0s - loss: 0.0050 - val_loss: 0.0497 - 80ms/epoch - 7ms/step\n",
            "Epoch 79/1000\n",
            "11/11 - 0s - loss: 0.0050 - val_loss: 0.0473 - 82ms/epoch - 7ms/step\n",
            "Epoch 80/1000\n",
            "11/11 - 0s - loss: 0.0049 - val_loss: 0.0483 - 78ms/epoch - 7ms/step\n",
            "Epoch 81/1000\n",
            "11/11 - 0s - loss: 0.0048 - val_loss: 0.0461 - 70ms/epoch - 6ms/step\n",
            "Epoch 82/1000\n",
            "11/11 - 0s - loss: 0.0049 - val_loss: 0.0423 - 81ms/epoch - 7ms/step\n",
            "Epoch 83/1000\n",
            "11/11 - 0s - loss: 0.0047 - val_loss: 0.0426 - 84ms/epoch - 8ms/step\n",
            "Epoch 84/1000\n",
            "11/11 - 0s - loss: 0.0046 - val_loss: 0.0376 - 86ms/epoch - 8ms/step\n",
            "Epoch 85/1000\n",
            "11/11 - 0s - loss: 0.0046 - val_loss: 0.0386 - 82ms/epoch - 7ms/step\n",
            "Epoch 86/1000\n",
            "11/11 - 0s - loss: 0.0043 - val_loss: 0.0418 - 78ms/epoch - 7ms/step\n",
            "Epoch 87/1000\n",
            "11/11 - 0s - loss: 0.0043 - val_loss: 0.0465 - 82ms/epoch - 7ms/step\n",
            "Epoch 88/1000\n",
            "11/11 - 0s - loss: 0.0043 - val_loss: 0.0440 - 76ms/epoch - 7ms/step\n",
            "Epoch 89/1000\n",
            "11/11 - 0s - loss: 0.0041 - val_loss: 0.0399 - 85ms/epoch - 8ms/step\n",
            "Epoch 90/1000\n",
            "11/11 - 0s - loss: 0.0040 - val_loss: 0.0373 - 81ms/epoch - 7ms/step\n",
            "Epoch 91/1000\n",
            "11/11 - 0s - loss: 0.0040 - val_loss: 0.0356 - 78ms/epoch - 7ms/step\n",
            "Epoch 92/1000\n",
            "11/11 - 0s - loss: 0.0040 - val_loss: 0.0361 - 75ms/epoch - 7ms/step\n",
            "Epoch 93/1000\n",
            "11/11 - 0s - loss: 0.0038 - val_loss: 0.0409 - 74ms/epoch - 7ms/step\n",
            "Epoch 94/1000\n",
            "11/11 - 0s - loss: 0.0038 - val_loss: 0.0392 - 83ms/epoch - 8ms/step\n",
            "Epoch 95/1000\n",
            "11/11 - 0s - loss: 0.0037 - val_loss: 0.0364 - 79ms/epoch - 7ms/step\n",
            "Epoch 96/1000\n",
            "11/11 - 0s - loss: 0.0036 - val_loss: 0.0364 - 79ms/epoch - 7ms/step\n",
            "Epoch 97/1000\n",
            "11/11 - 0s - loss: 0.0036 - val_loss: 0.0361 - 82ms/epoch - 7ms/step\n",
            "Epoch 98/1000\n",
            "11/11 - 0s - loss: 0.0036 - val_loss: 0.0358 - 88ms/epoch - 8ms/step\n",
            "Epoch 99/1000\n",
            "11/11 - 0s - loss: 0.0035 - val_loss: 0.0338 - 74ms/epoch - 7ms/step\n",
            "Epoch 100/1000\n",
            "11/11 - 0s - loss: 0.0035 - val_loss: 0.0340 - 82ms/epoch - 7ms/step\n",
            "Epoch 101/1000\n",
            "11/11 - 0s - loss: 0.0034 - val_loss: 0.0337 - 81ms/epoch - 7ms/step\n",
            "Epoch 102/1000\n",
            "11/11 - 0s - loss: 0.0034 - val_loss: 0.0330 - 74ms/epoch - 7ms/step\n",
            "Epoch 103/1000\n",
            "11/11 - 0s - loss: 0.0034 - val_loss: 0.0317 - 71ms/epoch - 6ms/step\n",
            "Epoch 104/1000\n",
            "11/11 - 0s - loss: 0.0033 - val_loss: 0.0326 - 75ms/epoch - 7ms/step\n",
            "Epoch 105/1000\n",
            "11/11 - 0s - loss: 0.0033 - val_loss: 0.0341 - 74ms/epoch - 7ms/step\n",
            "Epoch 106/1000\n",
            "11/11 - 0s - loss: 0.0032 - val_loss: 0.0314 - 72ms/epoch - 7ms/step\n",
            "Epoch 107/1000\n",
            "11/11 - 0s - loss: 0.0032 - val_loss: 0.0319 - 77ms/epoch - 7ms/step\n",
            "Epoch 108/1000\n",
            "11/11 - 0s - loss: 0.0031 - val_loss: 0.0317 - 76ms/epoch - 7ms/step\n",
            "Epoch 109/1000\n",
            "11/11 - 0s - loss: 0.0031 - val_loss: 0.0286 - 77ms/epoch - 7ms/step\n",
            "Epoch 110/1000\n",
            "11/11 - 0s - loss: 0.0031 - val_loss: 0.0299 - 70ms/epoch - 6ms/step\n",
            "Epoch 111/1000\n",
            "11/11 - 0s - loss: 0.0030 - val_loss: 0.0315 - 79ms/epoch - 7ms/step\n",
            "Epoch 112/1000\n",
            "11/11 - 0s - loss: 0.0030 - val_loss: 0.0298 - 78ms/epoch - 7ms/step\n",
            "Epoch 113/1000\n",
            "11/11 - 0s - loss: 0.0029 - val_loss: 0.0285 - 68ms/epoch - 6ms/step\n",
            "Epoch 114/1000\n",
            "11/11 - 0s - loss: 0.0029 - val_loss: 0.0273 - 79ms/epoch - 7ms/step\n",
            "Epoch 115/1000\n",
            "11/11 - 0s - loss: 0.0029 - val_loss: 0.0282 - 77ms/epoch - 7ms/step\n",
            "Epoch 116/1000\n",
            "11/11 - 0s - loss: 0.0029 - val_loss: 0.0295 - 76ms/epoch - 7ms/step\n",
            "Epoch 117/1000\n",
            "11/11 - 0s - loss: 0.0028 - val_loss: 0.0291 - 89ms/epoch - 8ms/step\n",
            "Epoch 118/1000\n",
            "11/11 - 0s - loss: 0.0028 - val_loss: 0.0283 - 77ms/epoch - 7ms/step\n",
            "Epoch 119/1000\n",
            "11/11 - 0s - loss: 0.0028 - val_loss: 0.0274 - 78ms/epoch - 7ms/step\n",
            "Epoch 120/1000\n",
            "11/11 - 0s - loss: 0.0027 - val_loss: 0.0281 - 87ms/epoch - 8ms/step\n",
            "Epoch 121/1000\n",
            "11/11 - 0s - loss: 0.0028 - val_loss: 0.0296 - 77ms/epoch - 7ms/step\n",
            "Epoch 122/1000\n",
            "11/11 - 0s - loss: 0.0027 - val_loss: 0.0270 - 75ms/epoch - 7ms/step\n",
            "Epoch 123/1000\n",
            "11/11 - 0s - loss: 0.0027 - val_loss: 0.0275 - 75ms/epoch - 7ms/step\n",
            "Epoch 124/1000\n",
            "11/11 - 0s - loss: 0.0027 - val_loss: 0.0266 - 71ms/epoch - 6ms/step\n",
            "Epoch 125/1000\n",
            "11/11 - 0s - loss: 0.0026 - val_loss: 0.0214 - 73ms/epoch - 7ms/step\n",
            "Epoch 126/1000\n",
            "11/11 - 0s - loss: 0.0026 - val_loss: 0.0246 - 79ms/epoch - 7ms/step\n",
            "Epoch 127/1000\n",
            "11/11 - 0s - loss: 0.0026 - val_loss: 0.0267 - 76ms/epoch - 7ms/step\n",
            "Epoch 128/1000\n",
            "11/11 - 0s - loss: 0.0026 - val_loss: 0.0242 - 73ms/epoch - 7ms/step\n",
            "Epoch 129/1000\n",
            "11/11 - 0s - loss: 0.0025 - val_loss: 0.0246 - 78ms/epoch - 7ms/step\n",
            "Epoch 130/1000\n",
            "11/11 - 0s - loss: 0.0025 - val_loss: 0.0239 - 78ms/epoch - 7ms/step\n",
            "Epoch 131/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0214 - 82ms/epoch - 7ms/step\n",
            "Epoch 132/1000\n",
            "11/11 - 0s - loss: 0.0025 - val_loss: 0.0223 - 71ms/epoch - 6ms/step\n",
            "Epoch 133/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0248 - 78ms/epoch - 7ms/step\n",
            "Epoch 134/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0236 - 84ms/epoch - 8ms/step\n",
            "Epoch 135/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0224 - 72ms/epoch - 7ms/step\n",
            "Epoch 136/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0226 - 88ms/epoch - 8ms/step\n",
            "Epoch 137/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0215 - 85ms/epoch - 8ms/step\n",
            "Epoch 138/1000\n",
            "11/11 - 0s - loss: 0.0023 - val_loss: 0.0232 - 75ms/epoch - 7ms/step\n",
            "Epoch 139/1000\n",
            "11/11 - 0s - loss: 0.0023 - val_loss: 0.0219 - 77ms/epoch - 7ms/step\n",
            "Epoch 140/1000\n",
            "11/11 - 0s - loss: 0.0023 - val_loss: 0.0224 - 76ms/epoch - 7ms/step\n",
            "Epoch 141/1000\n",
            "11/11 - 0s - loss: 0.0023 - val_loss: 0.0246 - 73ms/epoch - 7ms/step\n",
            "Epoch 142/1000\n",
            "11/11 - 0s - loss: 0.0024 - val_loss: 0.0227 - 82ms/epoch - 7ms/step\n",
            "Epoch 143/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0209 - 79ms/epoch - 7ms/step\n",
            "Epoch 144/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0218 - 78ms/epoch - 7ms/step\n",
            "Epoch 145/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0236 - 74ms/epoch - 7ms/step\n",
            "Epoch 146/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0221 - 88ms/epoch - 8ms/step\n",
            "Epoch 147/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0204 - 74ms/epoch - 7ms/step\n",
            "Epoch 148/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0199 - 78ms/epoch - 7ms/step\n",
            "Epoch 149/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0242 - 74ms/epoch - 7ms/step\n",
            "Epoch 150/1000\n",
            "11/11 - 0s - loss: 0.0022 - val_loss: 0.0210 - 101ms/epoch - 9ms/step\n",
            "Epoch 151/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0198 - 75ms/epoch - 7ms/step\n",
            "Epoch 152/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0202 - 78ms/epoch - 7ms/step\n",
            "Epoch 153/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0211 - 83ms/epoch - 8ms/step\n",
            "Epoch 154/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0220 - 81ms/epoch - 7ms/step\n",
            "Epoch 155/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0212 - 81ms/epoch - 7ms/step\n",
            "Epoch 156/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0207 - 84ms/epoch - 8ms/step\n",
            "Epoch 157/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0196 - 76ms/epoch - 7ms/step\n",
            "Epoch 158/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0187 - 81ms/epoch - 7ms/step\n",
            "Epoch 159/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0193 - 76ms/epoch - 7ms/step\n",
            "Epoch 160/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0204 - 77ms/epoch - 7ms/step\n",
            "Epoch 161/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0199 - 80ms/epoch - 7ms/step\n",
            "Epoch 162/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0211 - 83ms/epoch - 8ms/step\n",
            "Epoch 163/1000\n",
            "11/11 - 0s - loss: 0.0020 - val_loss: 0.0194 - 74ms/epoch - 7ms/step\n",
            "Epoch 164/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0181 - 85ms/epoch - 8ms/step\n",
            "Epoch 165/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0184 - 73ms/epoch - 7ms/step\n",
            "Epoch 166/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0184 - 76ms/epoch - 7ms/step\n",
            "Epoch 167/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0191 - 77ms/epoch - 7ms/step\n",
            "Epoch 168/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0183 - 89ms/epoch - 8ms/step\n",
            "Epoch 169/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0193 - 75ms/epoch - 7ms/step\n",
            "Epoch 170/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0198 - 84ms/epoch - 8ms/step\n",
            "Epoch 171/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0178 - 84ms/epoch - 8ms/step\n",
            "Epoch 172/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0139 - 84ms/epoch - 8ms/step\n",
            "Epoch 173/1000\n",
            "11/11 - 0s - loss: 0.0021 - val_loss: 0.0176 - 87ms/epoch - 8ms/step\n",
            "Epoch 174/1000\n",
            "11/11 - 0s - loss: 0.0019 - val_loss: 0.0196 - 78ms/epoch - 7ms/step\n",
            "Epoch 175/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0175 - 74ms/epoch - 7ms/step\n",
            "Epoch 176/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0179 - 77ms/epoch - 7ms/step\n",
            "Epoch 177/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0163 - 76ms/epoch - 7ms/step\n",
            "Epoch 178/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0169 - 75ms/epoch - 7ms/step\n",
            "Epoch 179/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0192 - 80ms/epoch - 7ms/step\n",
            "Epoch 180/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0175 - 78ms/epoch - 7ms/step\n",
            "Epoch 181/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0153 - 79ms/epoch - 7ms/step\n",
            "Epoch 182/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0165 - 90ms/epoch - 8ms/step\n",
            "Epoch 183/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0198 - 78ms/epoch - 7ms/step\n",
            "Epoch 184/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0176 - 72ms/epoch - 7ms/step\n",
            "Epoch 185/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0166 - 80ms/epoch - 7ms/step\n",
            "Epoch 186/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0174 - 81ms/epoch - 7ms/step\n",
            "Epoch 187/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0184 - 80ms/epoch - 7ms/step\n",
            "Epoch 188/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0197 - 80ms/epoch - 7ms/step\n",
            "Epoch 189/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0173 - 78ms/epoch - 7ms/step\n",
            "Epoch 190/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0178 - 81ms/epoch - 7ms/step\n",
            "Epoch 191/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0170 - 81ms/epoch - 7ms/step\n",
            "Epoch 192/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0171 - 77ms/epoch - 7ms/step\n",
            "Epoch 193/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0151 - 77ms/epoch - 7ms/step\n",
            "Epoch 194/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0188 - 85ms/epoch - 8ms/step\n",
            "Epoch 195/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0175 - 80ms/epoch - 7ms/step\n",
            "Epoch 196/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0162 - 84ms/epoch - 8ms/step\n",
            "Epoch 197/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0153 - 76ms/epoch - 7ms/step\n",
            "Epoch 198/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0179 - 78ms/epoch - 7ms/step\n",
            "Epoch 199/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0164 - 76ms/epoch - 7ms/step\n",
            "Epoch 200/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0136 - 76ms/epoch - 7ms/step\n",
            "Epoch 201/1000\n",
            "11/11 - 0s - loss: 0.0018 - val_loss: 0.0144 - 82ms/epoch - 7ms/step\n",
            "Epoch 202/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0186 - 78ms/epoch - 7ms/step\n",
            "Epoch 203/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0162 - 79ms/epoch - 7ms/step\n",
            "Epoch 204/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0154 - 72ms/epoch - 7ms/step\n",
            "Epoch 205/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0163 - 86ms/epoch - 8ms/step\n",
            "Epoch 206/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0162 - 83ms/epoch - 8ms/step\n",
            "Epoch 207/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0154 - 71ms/epoch - 6ms/step\n",
            "Epoch 208/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0150 - 73ms/epoch - 7ms/step\n",
            "Epoch 209/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0149 - 84ms/epoch - 8ms/step\n",
            "Epoch 210/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0188 - 78ms/epoch - 7ms/step\n",
            "Epoch 211/1000\n",
            "11/11 - 0s - loss: 0.0017 - val_loss: 0.0185 - 74ms/epoch - 7ms/step\n",
            "Epoch 212/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0152 - 78ms/epoch - 7ms/step\n",
            "Epoch 213/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0154 - 79ms/epoch - 7ms/step\n",
            "Epoch 214/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0157 - 77ms/epoch - 7ms/step\n",
            "Epoch 215/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0153 - 70ms/epoch - 6ms/step\n",
            "Epoch 216/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0137 - 85ms/epoch - 8ms/step\n",
            "Epoch 217/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0161 - 81ms/epoch - 7ms/step\n",
            "Epoch 218/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0121 - 84ms/epoch - 8ms/step\n",
            "Epoch 219/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0161 - 73ms/epoch - 7ms/step\n",
            "Epoch 220/1000\n",
            "11/11 - 0s - loss: 0.0016 - val_loss: 0.0171 - 85ms/epoch - 8ms/step\n",
            "Epoch 221/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0142 - 71ms/epoch - 6ms/step\n",
            "Epoch 222/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0134 - 83ms/epoch - 8ms/step\n",
            "Epoch 223/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0177 - 83ms/epoch - 8ms/step\n",
            "Epoch 224/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0148 - 81ms/epoch - 7ms/step\n",
            "Epoch 225/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0146 - 83ms/epoch - 8ms/step\n",
            "Epoch 226/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0140 - 76ms/epoch - 7ms/step\n",
            "Epoch 227/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0150 - 82ms/epoch - 7ms/step\n",
            "Epoch 228/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0161 - 80ms/epoch - 7ms/step\n",
            "Epoch 229/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0158 - 74ms/epoch - 7ms/step\n",
            "Epoch 230/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0135 - 90ms/epoch - 8ms/step\n",
            "Epoch 231/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0141 - 84ms/epoch - 8ms/step\n",
            "Epoch 232/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0144 - 75ms/epoch - 7ms/step\n",
            "Epoch 233/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0161 - 76ms/epoch - 7ms/step\n",
            "Epoch 234/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0154 - 78ms/epoch - 7ms/step\n",
            "Epoch 235/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0154 - 76ms/epoch - 7ms/step\n",
            "Epoch 236/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0142 - 74ms/epoch - 7ms/step\n",
            "Epoch 237/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0143 - 74ms/epoch - 7ms/step\n",
            "Epoch 238/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0145 - 82ms/epoch - 7ms/step\n",
            "Epoch 239/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0120 - 79ms/epoch - 7ms/step\n",
            "Epoch 240/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0153 - 79ms/epoch - 7ms/step\n",
            "Epoch 241/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0161 - 81ms/epoch - 7ms/step\n",
            "Epoch 242/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0130 - 91ms/epoch - 8ms/step\n",
            "Epoch 243/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0146 - 75ms/epoch - 7ms/step\n",
            "Epoch 244/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0166 - 81ms/epoch - 7ms/step\n",
            "Epoch 245/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0155 - 72ms/epoch - 7ms/step\n",
            "Epoch 246/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0159 - 72ms/epoch - 7ms/step\n",
            "Epoch 247/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0142 - 81ms/epoch - 7ms/step\n",
            "Epoch 248/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0155 - 81ms/epoch - 7ms/step\n",
            "Epoch 249/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0129 - 79ms/epoch - 7ms/step\n",
            "Epoch 250/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0143 - 73ms/epoch - 7ms/step\n",
            "Epoch 251/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0145 - 73ms/epoch - 7ms/step\n",
            "Epoch 252/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0137 - 78ms/epoch - 7ms/step\n",
            "Epoch 253/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0144 - 76ms/epoch - 7ms/step\n",
            "Epoch 254/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0133 - 93ms/epoch - 8ms/step\n",
            "Epoch 255/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0120 - 83ms/epoch - 8ms/step\n",
            "Epoch 256/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0159 - 77ms/epoch - 7ms/step\n",
            "Epoch 257/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0140 - 77ms/epoch - 7ms/step\n",
            "Epoch 258/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0130 - 77ms/epoch - 7ms/step\n",
            "Epoch 259/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0138 - 73ms/epoch - 7ms/step\n",
            "Epoch 260/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0151 - 74ms/epoch - 7ms/step\n",
            "Epoch 261/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0131 - 81ms/epoch - 7ms/step\n",
            "Epoch 262/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0142 - 83ms/epoch - 8ms/step\n",
            "Epoch 263/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0129 - 77ms/epoch - 7ms/step\n",
            "Epoch 264/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0143 - 74ms/epoch - 7ms/step\n",
            "Epoch 265/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0183 - 82ms/epoch - 7ms/step\n",
            "Epoch 266/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0133 - 88ms/epoch - 8ms/step\n",
            "Epoch 267/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0140 - 77ms/epoch - 7ms/step\n",
            "Epoch 268/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0176 - 73ms/epoch - 7ms/step\n",
            "Epoch 269/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0135 - 81ms/epoch - 7ms/step\n",
            "Epoch 270/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0139 - 100ms/epoch - 9ms/step\n",
            "Epoch 271/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0149 - 76ms/epoch - 7ms/step\n",
            "Epoch 272/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0139 - 76ms/epoch - 7ms/step\n",
            "Epoch 273/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0123 - 72ms/epoch - 7ms/step\n",
            "Epoch 274/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0143 - 82ms/epoch - 7ms/step\n",
            "Epoch 275/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0129 - 83ms/epoch - 8ms/step\n",
            "Epoch 276/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0137 - 82ms/epoch - 7ms/step\n",
            "Epoch 277/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0127 - 80ms/epoch - 7ms/step\n",
            "Epoch 278/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0137 - 79ms/epoch - 7ms/step\n",
            "Epoch 279/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0128 - 74ms/epoch - 7ms/step\n",
            "Epoch 280/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0123 - 83ms/epoch - 8ms/step\n",
            "Epoch 281/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0142 - 81ms/epoch - 7ms/step\n",
            "Epoch 282/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0133 - 75ms/epoch - 7ms/step\n",
            "Epoch 283/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0118 - 81ms/epoch - 7ms/step\n",
            "Epoch 284/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0126 - 89ms/epoch - 8ms/step\n",
            "Epoch 285/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0143 - 83ms/epoch - 8ms/step\n",
            "Epoch 286/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0134 - 77ms/epoch - 7ms/step\n",
            "Epoch 287/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0124 - 74ms/epoch - 7ms/step\n",
            "Epoch 288/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0140 - 78ms/epoch - 7ms/step\n",
            "Epoch 289/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0154 - 81ms/epoch - 7ms/step\n",
            "Epoch 290/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0108 - 90ms/epoch - 8ms/step\n",
            "Epoch 291/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0139 - 95ms/epoch - 9ms/step\n",
            "Epoch 292/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0132 - 80ms/epoch - 7ms/step\n",
            "Epoch 293/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0124 - 78ms/epoch - 7ms/step\n",
            "Epoch 294/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0133 - 74ms/epoch - 7ms/step\n",
            "Epoch 295/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0113 - 81ms/epoch - 7ms/step\n",
            "Epoch 296/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0125 - 81ms/epoch - 7ms/step\n",
            "Epoch 297/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0139 - 75ms/epoch - 7ms/step\n",
            "Epoch 298/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0134 - 81ms/epoch - 7ms/step\n",
            "Epoch 299/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0160 - 80ms/epoch - 7ms/step\n",
            "Epoch 300/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0115 - 83ms/epoch - 8ms/step\n",
            "Epoch 301/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0120 - 81ms/epoch - 7ms/step\n",
            "Epoch 302/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0129 - 90ms/epoch - 8ms/step\n",
            "Epoch 303/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0126 - 76ms/epoch - 7ms/step\n",
            "Epoch 304/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0122 - 85ms/epoch - 8ms/step\n",
            "Epoch 305/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0120 - 77ms/epoch - 7ms/step\n",
            "Epoch 306/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0138 - 78ms/epoch - 7ms/step\n",
            "Epoch 307/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0126 - 75ms/epoch - 7ms/step\n",
            "Epoch 308/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0110 - 80ms/epoch - 7ms/step\n",
            "Epoch 309/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0123 - 78ms/epoch - 7ms/step\n",
            "Epoch 310/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0127 - 71ms/epoch - 6ms/step\n",
            "Epoch 311/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0124 - 74ms/epoch - 7ms/step\n",
            "Epoch 312/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0123 - 83ms/epoch - 8ms/step\n",
            "Epoch 313/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0132 - 75ms/epoch - 7ms/step\n",
            "Epoch 314/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0116 - 81ms/epoch - 7ms/step\n",
            "Epoch 315/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0125 - 79ms/epoch - 7ms/step\n",
            "Epoch 316/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0110 - 101ms/epoch - 9ms/step\n",
            "Epoch 317/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0134 - 79ms/epoch - 7ms/step\n",
            "Epoch 318/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0121 - 74ms/epoch - 7ms/step\n",
            "Epoch 319/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0121 - 86ms/epoch - 8ms/step\n",
            "Epoch 320/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0122 - 84ms/epoch - 8ms/step\n",
            "Epoch 321/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0096 - 88ms/epoch - 8ms/step\n",
            "Epoch 322/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0137 - 81ms/epoch - 7ms/step\n",
            "Epoch 323/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0118 - 86ms/epoch - 8ms/step\n",
            "Epoch 324/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0116 - 80ms/epoch - 7ms/step\n",
            "Epoch 325/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 88ms/epoch - 8ms/step\n",
            "Epoch 326/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0122 - 76ms/epoch - 7ms/step\n",
            "Epoch 327/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0111 - 78ms/epoch - 7ms/step\n",
            "Epoch 328/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0132 - 80ms/epoch - 7ms/step\n",
            "Epoch 329/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0125 - 74ms/epoch - 7ms/step\n",
            "Epoch 330/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0117 - 79ms/epoch - 7ms/step\n",
            "Epoch 331/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0123 - 78ms/epoch - 7ms/step\n",
            "Epoch 332/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0123 - 102ms/epoch - 9ms/step\n",
            "Epoch 333/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0108 - 82ms/epoch - 7ms/step\n",
            "Epoch 334/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0101 - 74ms/epoch - 7ms/step\n",
            "Epoch 335/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0134 - 79ms/epoch - 7ms/step\n",
            "Epoch 336/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0105 - 74ms/epoch - 7ms/step\n",
            "Epoch 337/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0115 - 90ms/epoch - 8ms/step\n",
            "Epoch 338/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0139 - 88ms/epoch - 8ms/step\n",
            "Epoch 339/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0099 - 85ms/epoch - 8ms/step\n",
            "Epoch 340/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0120 - 80ms/epoch - 7ms/step\n",
            "Epoch 341/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0132 - 79ms/epoch - 7ms/step\n",
            "Epoch 342/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0115 - 81ms/epoch - 7ms/step\n",
            "Epoch 343/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 78ms/epoch - 7ms/step\n",
            "Epoch 344/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0106 - 81ms/epoch - 7ms/step\n",
            "Epoch 345/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0140 - 75ms/epoch - 7ms/step\n",
            "Epoch 346/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0110 - 92ms/epoch - 8ms/step\n",
            "Epoch 347/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 93ms/epoch - 8ms/step\n",
            "Epoch 348/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0125 - 84ms/epoch - 8ms/step\n",
            "Epoch 349/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0098 - 100ms/epoch - 9ms/step\n",
            "Epoch 350/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0111 - 77ms/epoch - 7ms/step\n",
            "Epoch 351/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0131 - 80ms/epoch - 7ms/step\n",
            "Epoch 352/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0108 - 79ms/epoch - 7ms/step\n",
            "Epoch 353/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0110 - 76ms/epoch - 7ms/step\n",
            "Epoch 354/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0129 - 75ms/epoch - 7ms/step\n",
            "Epoch 355/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0105 - 77ms/epoch - 7ms/step\n",
            "Epoch 356/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0118 - 76ms/epoch - 7ms/step\n",
            "Epoch 357/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0108 - 72ms/epoch - 7ms/step\n",
            "Epoch 358/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0130 - 77ms/epoch - 7ms/step\n",
            "Epoch 359/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0114 - 75ms/epoch - 7ms/step\n",
            "Epoch 360/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 75ms/epoch - 7ms/step\n",
            "Epoch 361/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0129 - 84ms/epoch - 8ms/step\n",
            "Epoch 362/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0104 - 76ms/epoch - 7ms/step\n",
            "Epoch 363/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 84ms/epoch - 8ms/step\n",
            "Epoch 364/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0110 - 77ms/epoch - 7ms/step\n",
            "Epoch 365/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0112 - 78ms/epoch - 7ms/step\n",
            "Epoch 366/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0127 - 80ms/epoch - 7ms/step\n",
            "Epoch 367/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0114 - 79ms/epoch - 7ms/step\n",
            "Epoch 368/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0141 - 83ms/epoch - 8ms/step\n",
            "Epoch 369/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0113 - 80ms/epoch - 7ms/step\n",
            "Epoch 370/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0126 - 73ms/epoch - 7ms/step\n",
            "Epoch 371/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0129 - 74ms/epoch - 7ms/step\n",
            "Epoch 372/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0105 - 82ms/epoch - 7ms/step\n",
            "Epoch 373/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0133 - 90ms/epoch - 8ms/step\n",
            "Epoch 374/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0122 - 78ms/epoch - 7ms/step\n",
            "Epoch 375/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0114 - 79ms/epoch - 7ms/step\n",
            "Epoch 376/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0124 - 70ms/epoch - 6ms/step\n",
            "Epoch 377/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0121 - 76ms/epoch - 7ms/step\n",
            "Epoch 378/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0127 - 91ms/epoch - 8ms/step\n",
            "Epoch 379/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0109 - 77ms/epoch - 7ms/step\n",
            "Epoch 380/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0135 - 75ms/epoch - 7ms/step\n",
            "Epoch 381/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0104 - 69ms/epoch - 6ms/step\n",
            "Epoch 382/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0118 - 78ms/epoch - 7ms/step\n",
            "Epoch 383/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0121 - 74ms/epoch - 7ms/step\n",
            "Epoch 384/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0119 - 72ms/epoch - 7ms/step\n",
            "Epoch 385/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0135 - 85ms/epoch - 8ms/step\n",
            "Epoch 386/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0104 - 84ms/epoch - 8ms/step\n",
            "Epoch 387/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0122 - 89ms/epoch - 8ms/step\n",
            "Epoch 388/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0108 - 88ms/epoch - 8ms/step\n",
            "Epoch 389/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0101 - 81ms/epoch - 7ms/step\n",
            "Epoch 390/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0125 - 73ms/epoch - 7ms/step\n",
            "Epoch 391/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0102 - 79ms/epoch - 7ms/step\n",
            "Epoch 392/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 81ms/epoch - 7ms/step\n",
            "Epoch 393/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0114 - 82ms/epoch - 7ms/step\n",
            "Epoch 394/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0112 - 83ms/epoch - 8ms/step\n",
            "Epoch 395/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0106 - 78ms/epoch - 7ms/step\n",
            "Epoch 396/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0115 - 78ms/epoch - 7ms/step\n",
            "Epoch 397/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0125 - 79ms/epoch - 7ms/step\n",
            "Epoch 398/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0132 - 76ms/epoch - 7ms/step\n",
            "Epoch 399/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0119 - 70ms/epoch - 6ms/step\n",
            "Epoch 400/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0103 - 89ms/epoch - 8ms/step\n",
            "Epoch 401/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0121 - 76ms/epoch - 7ms/step\n",
            "Epoch 402/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0108 - 82ms/epoch - 7ms/step\n",
            "Epoch 403/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0108 - 74ms/epoch - 7ms/step\n",
            "Epoch 404/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0121 - 77ms/epoch - 7ms/step\n",
            "Epoch 405/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0110 - 80ms/epoch - 7ms/step\n",
            "Epoch 406/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0109 - 78ms/epoch - 7ms/step\n",
            "Epoch 407/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 83ms/epoch - 8ms/step\n",
            "Epoch 408/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0117 - 88ms/epoch - 8ms/step\n",
            "Epoch 409/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0128 - 79ms/epoch - 7ms/step\n",
            "Epoch 410/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0094 - 79ms/epoch - 7ms/step\n",
            "Epoch 411/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0111 - 74ms/epoch - 7ms/step\n",
            "Epoch 412/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0103 - 81ms/epoch - 7ms/step\n",
            "Epoch 413/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0151 - 72ms/epoch - 7ms/step\n",
            "Epoch 414/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0077 - 78ms/epoch - 7ms/step\n",
            "Epoch 415/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0118 - 83ms/epoch - 8ms/step\n",
            "Epoch 416/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0129 - 82ms/epoch - 7ms/step\n",
            "Epoch 417/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0120 - 75ms/epoch - 7ms/step\n",
            "Epoch 418/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0101 - 81ms/epoch - 7ms/step\n",
            "Epoch 419/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0119 - 79ms/epoch - 7ms/step\n",
            "Epoch 420/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0104 - 89ms/epoch - 8ms/step\n",
            "Epoch 421/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0118 - 84ms/epoch - 8ms/step\n",
            "Epoch 422/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0100 - 83ms/epoch - 8ms/step\n",
            "Epoch 423/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0114 - 81ms/epoch - 7ms/step\n",
            "Epoch 424/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 82ms/epoch - 7ms/step\n",
            "Epoch 425/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 81ms/epoch - 7ms/step\n",
            "Epoch 426/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0116 - 79ms/epoch - 7ms/step\n",
            "Epoch 427/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0106 - 72ms/epoch - 7ms/step\n",
            "Epoch 428/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 82ms/epoch - 7ms/step\n",
            "Epoch 429/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0091 - 82ms/epoch - 7ms/step\n",
            "Epoch 430/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 85ms/epoch - 8ms/step\n",
            "Epoch 431/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 85ms/epoch - 8ms/step\n",
            "Epoch 432/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0103 - 81ms/epoch - 7ms/step\n",
            "Epoch 433/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0137 - 81ms/epoch - 7ms/step\n",
            "Epoch 434/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0090 - 83ms/epoch - 8ms/step\n",
            "Epoch 435/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0121 - 88ms/epoch - 8ms/step\n",
            "Epoch 436/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0110 - 79ms/epoch - 7ms/step\n",
            "Epoch 437/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 72ms/epoch - 7ms/step\n",
            "Epoch 438/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 76ms/epoch - 7ms/step\n",
            "Epoch 439/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0103 - 86ms/epoch - 8ms/step\n",
            "Epoch 440/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 83ms/epoch - 8ms/step\n",
            "Epoch 441/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 79ms/epoch - 7ms/step\n",
            "Epoch 442/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 77ms/epoch - 7ms/step\n",
            "Epoch 443/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0106 - 75ms/epoch - 7ms/step\n",
            "Epoch 444/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 81ms/epoch - 7ms/step\n",
            "Epoch 445/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0105 - 84ms/epoch - 8ms/step\n",
            "Epoch 446/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0100 - 79ms/epoch - 7ms/step\n",
            "Epoch 447/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0100 - 85ms/epoch - 8ms/step\n",
            "Epoch 448/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 74ms/epoch - 7ms/step\n",
            "Epoch 449/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 76ms/epoch - 7ms/step\n",
            "Epoch 450/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 77ms/epoch - 7ms/step\n",
            "Epoch 451/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0108 - 94ms/epoch - 9ms/step\n",
            "Epoch 452/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0104 - 81ms/epoch - 7ms/step\n",
            "Epoch 453/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0119 - 78ms/epoch - 7ms/step\n",
            "Epoch 454/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0094 - 81ms/epoch - 7ms/step\n",
            "Epoch 455/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0092 - 81ms/epoch - 7ms/step\n",
            "Epoch 456/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0097 - 83ms/epoch - 8ms/step\n",
            "Epoch 457/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0117 - 74ms/epoch - 7ms/step\n",
            "Epoch 458/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0097 - 86ms/epoch - 8ms/step\n",
            "Epoch 459/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 84ms/epoch - 8ms/step\n",
            "Epoch 460/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0099 - 82ms/epoch - 7ms/step\n",
            "Epoch 461/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 78ms/epoch - 7ms/step\n",
            "Epoch 462/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0104 - 74ms/epoch - 7ms/step\n",
            "Epoch 463/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0105 - 76ms/epoch - 7ms/step\n",
            "Epoch 464/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 76ms/epoch - 7ms/step\n",
            "Epoch 465/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 81ms/epoch - 7ms/step\n",
            "Epoch 466/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0088 - 79ms/epoch - 7ms/step\n",
            "Epoch 467/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0106 - 83ms/epoch - 8ms/step\n",
            "Epoch 468/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 78ms/epoch - 7ms/step\n",
            "Epoch 469/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0103 - 74ms/epoch - 7ms/step\n",
            "Epoch 470/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0091 - 85ms/epoch - 8ms/step\n",
            "Epoch 471/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0099 - 75ms/epoch - 7ms/step\n",
            "Epoch 472/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 81ms/epoch - 7ms/step\n",
            "Epoch 473/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 90ms/epoch - 8ms/step\n",
            "Epoch 474/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0107 - 74ms/epoch - 7ms/step\n",
            "Epoch 475/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0106 - 81ms/epoch - 7ms/step\n",
            "Epoch 476/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0119 - 79ms/epoch - 7ms/step\n",
            "Epoch 477/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0090 - 82ms/epoch - 7ms/step\n",
            "Epoch 478/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0119 - 76ms/epoch - 7ms/step\n",
            "Epoch 479/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0090 - 79ms/epoch - 7ms/step\n",
            "Epoch 480/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0092 - 89ms/epoch - 8ms/step\n",
            "Epoch 481/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0084 - 83ms/epoch - 8ms/step\n",
            "Epoch 482/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0105 - 77ms/epoch - 7ms/step\n",
            "Epoch 483/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0088 - 78ms/epoch - 7ms/step\n",
            "Epoch 484/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0104 - 78ms/epoch - 7ms/step\n",
            "Epoch 485/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0091 - 79ms/epoch - 7ms/step\n",
            "Epoch 486/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0106 - 81ms/epoch - 7ms/step\n",
            "Epoch 487/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0101 - 79ms/epoch - 7ms/step\n",
            "Epoch 488/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0126 - 74ms/epoch - 7ms/step\n",
            "Epoch 489/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0094 - 85ms/epoch - 8ms/step\n",
            "Epoch 490/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 82ms/epoch - 7ms/step\n",
            "Epoch 491/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0105 - 85ms/epoch - 8ms/step\n",
            "Epoch 492/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0108 - 75ms/epoch - 7ms/step\n",
            "Epoch 493/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 81ms/epoch - 7ms/step\n",
            "Epoch 494/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 75ms/epoch - 7ms/step\n",
            "Epoch 495/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0090 - 86ms/epoch - 8ms/step\n",
            "Epoch 496/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 81ms/epoch - 7ms/step\n",
            "Epoch 497/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0083 - 80ms/epoch - 7ms/step\n",
            "Epoch 498/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 77ms/epoch - 7ms/step\n",
            "Epoch 499/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0108 - 76ms/epoch - 7ms/step\n",
            "Epoch 500/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 77ms/epoch - 7ms/step\n",
            "Epoch 501/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0108 - 84ms/epoch - 8ms/step\n",
            "Epoch 502/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0096 - 79ms/epoch - 7ms/step\n",
            "Epoch 503/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0100 - 89ms/epoch - 8ms/step\n",
            "Epoch 504/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0091 - 75ms/epoch - 7ms/step\n",
            "Epoch 505/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0104 - 99ms/epoch - 9ms/step\n",
            "Epoch 506/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0091 - 84ms/epoch - 8ms/step\n",
            "Epoch 507/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0111 - 76ms/epoch - 7ms/step\n",
            "Epoch 508/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 77ms/epoch - 7ms/step\n",
            "Epoch 509/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0121 - 72ms/epoch - 7ms/step\n",
            "Epoch 510/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0093 - 74ms/epoch - 7ms/step\n",
            "Epoch 511/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0090 - 83ms/epoch - 8ms/step\n",
            "Epoch 512/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0112 - 80ms/epoch - 7ms/step\n",
            "Epoch 513/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0096 - 79ms/epoch - 7ms/step\n",
            "Epoch 514/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0099 - 74ms/epoch - 7ms/step\n",
            "Epoch 515/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0097 - 82ms/epoch - 7ms/step\n",
            "Epoch 516/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0100 - 81ms/epoch - 7ms/step\n",
            "Epoch 517/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0083 - 91ms/epoch - 8ms/step\n",
            "Epoch 518/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0105 - 85ms/epoch - 8ms/step\n",
            "Epoch 519/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 77ms/epoch - 7ms/step\n",
            "Epoch 520/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 77ms/epoch - 7ms/step\n",
            "Epoch 521/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 79ms/epoch - 7ms/step\n",
            "Epoch 522/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0096 - 80ms/epoch - 7ms/step\n",
            "Epoch 523/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0098 - 78ms/epoch - 7ms/step\n",
            "Epoch 524/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0098 - 78ms/epoch - 7ms/step\n",
            "Epoch 525/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 76ms/epoch - 7ms/step\n",
            "Epoch 526/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0102 - 86ms/epoch - 8ms/step\n",
            "Epoch 527/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0086 - 86ms/epoch - 8ms/step\n",
            "Epoch 528/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0109 - 81ms/epoch - 7ms/step\n",
            "Epoch 529/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 78ms/epoch - 7ms/step\n",
            "Epoch 530/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0103 - 78ms/epoch - 7ms/step\n",
            "Epoch 531/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0096 - 84ms/epoch - 8ms/step\n",
            "Epoch 532/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0088 - 85ms/epoch - 8ms/step\n",
            "Epoch 533/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0127 - 83ms/epoch - 8ms/step\n",
            "Epoch 534/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 77ms/epoch - 7ms/step\n",
            "Epoch 535/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0100 - 85ms/epoch - 8ms/step\n",
            "Epoch 536/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0117 - 84ms/epoch - 8ms/step\n",
            "Epoch 537/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0079 - 77ms/epoch - 7ms/step\n",
            "Epoch 538/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0113 - 83ms/epoch - 8ms/step\n",
            "Epoch 539/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0099 - 79ms/epoch - 7ms/step\n",
            "Epoch 540/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0098 - 87ms/epoch - 8ms/step\n",
            "Epoch 541/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0078 - 85ms/epoch - 8ms/step\n",
            "Epoch 542/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0094 - 74ms/epoch - 7ms/step\n",
            "Epoch 543/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0103 - 78ms/epoch - 7ms/step\n",
            "Epoch 544/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 86ms/epoch - 8ms/step\n",
            "Epoch 545/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 77ms/epoch - 7ms/step\n",
            "Epoch 546/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 81ms/epoch - 7ms/step\n",
            "Epoch 547/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0098 - 72ms/epoch - 7ms/step\n",
            "Epoch 548/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0098 - 78ms/epoch - 7ms/step\n",
            "Epoch 549/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 87ms/epoch - 8ms/step\n",
            "Epoch 550/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 91ms/epoch - 8ms/step\n",
            "Epoch 551/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0102 - 81ms/epoch - 7ms/step\n",
            "Epoch 552/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 73ms/epoch - 7ms/step\n",
            "Epoch 553/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 77ms/epoch - 7ms/step\n",
            "Epoch 554/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 81ms/epoch - 7ms/step\n",
            "Epoch 555/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0097 - 79ms/epoch - 7ms/step\n",
            "Epoch 556/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0097 - 80ms/epoch - 7ms/step\n",
            "Epoch 557/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0094 - 80ms/epoch - 7ms/step\n",
            "Epoch 558/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0079 - 83ms/epoch - 8ms/step\n",
            "Epoch 559/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0097 - 71ms/epoch - 6ms/step\n",
            "Epoch 560/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 82ms/epoch - 7ms/step\n",
            "Epoch 561/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0114 - 82ms/epoch - 7ms/step\n",
            "Epoch 562/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0090 - 84ms/epoch - 8ms/step\n",
            "Epoch 563/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0115 - 93ms/epoch - 8ms/step\n",
            "Epoch 564/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0094 - 94ms/epoch - 9ms/step\n",
            "Epoch 565/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0095 - 82ms/epoch - 7ms/step\n",
            "Epoch 566/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0099 - 75ms/epoch - 7ms/step\n",
            "Epoch 567/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0097 - 77ms/epoch - 7ms/step\n",
            "Epoch 568/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0096 - 69ms/epoch - 6ms/step\n",
            "Epoch 569/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0079 - 84ms/epoch - 8ms/step\n",
            "Epoch 570/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0104 - 85ms/epoch - 8ms/step\n",
            "Epoch 571/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0087 - 90ms/epoch - 8ms/step\n",
            "Epoch 572/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0080 - 79ms/epoch - 7ms/step\n",
            "Epoch 573/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0110 - 75ms/epoch - 7ms/step\n",
            "Epoch 574/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 89ms/epoch - 8ms/step\n",
            "Epoch 575/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 75ms/epoch - 7ms/step\n",
            "Epoch 576/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0108 - 79ms/epoch - 7ms/step\n",
            "Epoch 577/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 78ms/epoch - 7ms/step\n",
            "Epoch 578/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 79ms/epoch - 7ms/step\n",
            "Epoch 579/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0105 - 80ms/epoch - 7ms/step\n",
            "Epoch 580/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 91ms/epoch - 8ms/step\n",
            "Epoch 581/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0099 - 75ms/epoch - 7ms/step\n",
            "Epoch 582/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 583/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0094 - 77ms/epoch - 7ms/step\n",
            "Epoch 584/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0099 - 78ms/epoch - 7ms/step\n",
            "Epoch 585/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0095 - 91ms/epoch - 8ms/step\n",
            "Epoch 586/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 81ms/epoch - 7ms/step\n",
            "Epoch 587/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0094 - 85ms/epoch - 8ms/step\n",
            "Epoch 588/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0086 - 78ms/epoch - 7ms/step\n",
            "Epoch 589/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0101 - 84ms/epoch - 8ms/step\n",
            "Epoch 590/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0095 - 81ms/epoch - 7ms/step\n",
            "Epoch 591/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 81ms/epoch - 7ms/step\n",
            "Epoch 592/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 77ms/epoch - 7ms/step\n",
            "Epoch 593/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0098 - 78ms/epoch - 7ms/step\n",
            "Epoch 594/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0088 - 85ms/epoch - 8ms/step\n",
            "Epoch 595/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0097 - 76ms/epoch - 7ms/step\n",
            "Epoch 596/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0097 - 74ms/epoch - 7ms/step\n",
            "Epoch 597/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0087 - 87ms/epoch - 8ms/step\n",
            "Epoch 598/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0103 - 79ms/epoch - 7ms/step\n",
            "Epoch 599/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0102 - 80ms/epoch - 7ms/step\n",
            "Epoch 600/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0101 - 74ms/epoch - 7ms/step\n",
            "Epoch 601/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 76ms/epoch - 7ms/step\n",
            "Epoch 602/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0102 - 82ms/epoch - 7ms/step\n",
            "Epoch 603/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 84ms/epoch - 8ms/step\n",
            "Epoch 604/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0097 - 78ms/epoch - 7ms/step\n",
            "Epoch 605/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 79ms/epoch - 7ms/step\n",
            "Epoch 606/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 78ms/epoch - 7ms/step\n",
            "Epoch 607/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0094 - 78ms/epoch - 7ms/step\n",
            "Epoch 608/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 75ms/epoch - 7ms/step\n",
            "Epoch 609/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0099 - 93ms/epoch - 8ms/step\n",
            "Epoch 610/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 75ms/epoch - 7ms/step\n",
            "Epoch 611/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0101 - 80ms/epoch - 7ms/step\n",
            "Epoch 612/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 85ms/epoch - 8ms/step\n",
            "Epoch 613/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 78ms/epoch - 7ms/step\n",
            "Epoch 614/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0103 - 71ms/epoch - 6ms/step\n",
            "Epoch 615/1000\n",
            "11/11 - 0s - loss: 9.8034e-04 - val_loss: 0.0070 - 81ms/epoch - 7ms/step\n",
            "Epoch 616/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0079 - 79ms/epoch - 7ms/step\n",
            "Epoch 617/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0078 - 80ms/epoch - 7ms/step\n",
            "Epoch 618/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0088 - 83ms/epoch - 8ms/step\n",
            "Epoch 619/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 78ms/epoch - 7ms/step\n",
            "Epoch 620/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0092 - 75ms/epoch - 7ms/step\n",
            "Epoch 621/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 87ms/epoch - 8ms/step\n",
            "Epoch 622/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 97ms/epoch - 9ms/step\n",
            "Epoch 623/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 78ms/epoch - 7ms/step\n",
            "Epoch 624/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 81ms/epoch - 7ms/step\n",
            "Epoch 625/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 86ms/epoch - 8ms/step\n",
            "Epoch 626/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 75ms/epoch - 7ms/step\n",
            "Epoch 627/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 628/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0103 - 76ms/epoch - 7ms/step\n",
            "Epoch 629/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 106ms/epoch - 10ms/step\n",
            "Epoch 630/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 87ms/epoch - 8ms/step\n",
            "Epoch 631/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 76ms/epoch - 7ms/step\n",
            "Epoch 632/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0085 - 87ms/epoch - 8ms/step\n",
            "Epoch 633/1000\n",
            "11/11 - 0s - loss: 9.9775e-04 - val_loss: 0.0087 - 75ms/epoch - 7ms/step\n",
            "Epoch 634/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0107 - 79ms/epoch - 7ms/step\n",
            "Epoch 635/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0110 - 76ms/epoch - 7ms/step\n",
            "Epoch 636/1000\n",
            "11/11 - 0s - loss: 9.9992e-04 - val_loss: 0.0089 - 73ms/epoch - 7ms/step\n",
            "Epoch 637/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0114 - 78ms/epoch - 7ms/step\n",
            "Epoch 638/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 77ms/epoch - 7ms/step\n",
            "Epoch 639/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 105ms/epoch - 10ms/step\n",
            "Epoch 640/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 78ms/epoch - 7ms/step\n",
            "Epoch 641/1000\n",
            "11/11 - 0s - loss: 9.9263e-04 - val_loss: 0.0086 - 76ms/epoch - 7ms/step\n",
            "Epoch 642/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0102 - 73ms/epoch - 7ms/step\n",
            "Epoch 643/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 82ms/epoch - 7ms/step\n",
            "Epoch 644/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 83ms/epoch - 8ms/step\n",
            "Epoch 645/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0098 - 68ms/epoch - 6ms/step\n",
            "Epoch 646/1000\n",
            "11/11 - 0s - loss: 9.7487e-04 - val_loss: 0.0075 - 82ms/epoch - 7ms/step\n",
            "Epoch 647/1000\n",
            "11/11 - 0s - loss: 9.9633e-04 - val_loss: 0.0091 - 86ms/epoch - 8ms/step\n",
            "Epoch 648/1000\n",
            "11/11 - 0s - loss: 9.8808e-04 - val_loss: 0.0068 - 86ms/epoch - 8ms/step\n",
            "Epoch 649/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0096 - 76ms/epoch - 7ms/step\n",
            "Epoch 650/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0091 - 76ms/epoch - 7ms/step\n",
            "Epoch 651/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 75ms/epoch - 7ms/step\n",
            "Epoch 652/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0104 - 77ms/epoch - 7ms/step\n",
            "Epoch 653/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0076 - 78ms/epoch - 7ms/step\n",
            "Epoch 654/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0082 - 79ms/epoch - 7ms/step\n",
            "Epoch 655/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 88ms/epoch - 8ms/step\n",
            "Epoch 656/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0084 - 86ms/epoch - 8ms/step\n",
            "Epoch 657/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0096 - 76ms/epoch - 7ms/step\n",
            "Epoch 658/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0096 - 81ms/epoch - 7ms/step\n",
            "Epoch 659/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0082 - 72ms/epoch - 7ms/step\n",
            "Epoch 660/1000\n",
            "11/11 - 0s - loss: 9.7245e-04 - val_loss: 0.0102 - 75ms/epoch - 7ms/step\n",
            "Epoch 661/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0084 - 77ms/epoch - 7ms/step\n",
            "Epoch 662/1000\n",
            "11/11 - 0s - loss: 9.8960e-04 - val_loss: 0.0099 - 82ms/epoch - 7ms/step\n",
            "Epoch 663/1000\n",
            "11/11 - 0s - loss: 9.9013e-04 - val_loss: 0.0090 - 84ms/epoch - 8ms/step\n",
            "Epoch 664/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0094 - 82ms/epoch - 7ms/step\n",
            "Epoch 665/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0084 - 80ms/epoch - 7ms/step\n",
            "Epoch 666/1000\n",
            "11/11 - 0s - loss: 9.9253e-04 - val_loss: 0.0087 - 82ms/epoch - 7ms/step\n",
            "Epoch 667/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 80ms/epoch - 7ms/step\n",
            "Epoch 668/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 81ms/epoch - 7ms/step\n",
            "Epoch 669/1000\n",
            "11/11 - 0s - loss: 9.8596e-04 - val_loss: 0.0086 - 78ms/epoch - 7ms/step\n",
            "Epoch 670/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0079 - 97ms/epoch - 9ms/step\n",
            "Epoch 671/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0100 - 98ms/epoch - 9ms/step\n",
            "Epoch 672/1000\n",
            "11/11 - 0s - loss: 9.9299e-04 - val_loss: 0.0080 - 71ms/epoch - 6ms/step\n",
            "Epoch 673/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0091 - 74ms/epoch - 7ms/step\n",
            "Epoch 674/1000\n",
            "11/11 - 0s - loss: 9.9716e-04 - val_loss: 0.0087 - 81ms/epoch - 7ms/step\n",
            "Epoch 675/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 80ms/epoch - 7ms/step\n",
            "Epoch 676/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0101 - 78ms/epoch - 7ms/step\n",
            "Epoch 677/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 87ms/epoch - 8ms/step\n",
            "Epoch 678/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 85ms/epoch - 8ms/step\n",
            "Epoch 679/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0088 - 83ms/epoch - 8ms/step\n",
            "Epoch 680/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 96ms/epoch - 9ms/step\n",
            "Epoch 681/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0088 - 77ms/epoch - 7ms/step\n",
            "Epoch 682/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0092 - 87ms/epoch - 8ms/step\n",
            "Epoch 683/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 80ms/epoch - 7ms/step\n",
            "Epoch 684/1000\n",
            "11/11 - 0s - loss: 9.9851e-04 - val_loss: 0.0081 - 76ms/epoch - 7ms/step\n",
            "Epoch 685/1000\n",
            "11/11 - 0s - loss: 9.8619e-04 - val_loss: 0.0079 - 83ms/epoch - 8ms/step\n",
            "Epoch 686/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0085 - 89ms/epoch - 8ms/step\n",
            "Epoch 687/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0104 - 79ms/epoch - 7ms/step\n",
            "Epoch 688/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 79ms/epoch - 7ms/step\n",
            "Epoch 689/1000\n",
            "11/11 - 0s - loss: 9.7566e-04 - val_loss: 0.0080 - 76ms/epoch - 7ms/step\n",
            "Epoch 690/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0087 - 81ms/epoch - 7ms/step\n",
            "Epoch 691/1000\n",
            "11/11 - 0s - loss: 9.8072e-04 - val_loss: 0.0082 - 91ms/epoch - 8ms/step\n",
            "Epoch 692/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0083 - 83ms/epoch - 8ms/step\n",
            "Epoch 693/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0097 - 85ms/epoch - 8ms/step\n",
            "Epoch 694/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0077 - 82ms/epoch - 7ms/step\n",
            "Epoch 695/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 77ms/epoch - 7ms/step\n",
            "Epoch 696/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 76ms/epoch - 7ms/step\n",
            "Epoch 697/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0085 - 89ms/epoch - 8ms/step\n",
            "Epoch 698/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 96ms/epoch - 9ms/step\n",
            "Epoch 699/1000\n",
            "11/11 - 0s - loss: 9.9416e-04 - val_loss: 0.0077 - 80ms/epoch - 7ms/step\n",
            "Epoch 700/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 80ms/epoch - 7ms/step\n",
            "Epoch 701/1000\n",
            "11/11 - 0s - loss: 9.7986e-04 - val_loss: 0.0078 - 84ms/epoch - 8ms/step\n",
            "Epoch 702/1000\n",
            "11/11 - 0s - loss: 9.8860e-04 - val_loss: 0.0088 - 84ms/epoch - 8ms/step\n",
            "Epoch 703/1000\n",
            "11/11 - 0s - loss: 9.9580e-04 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 704/1000\n",
            "11/11 - 0s - loss: 9.7624e-04 - val_loss: 0.0089 - 82ms/epoch - 7ms/step\n",
            "Epoch 705/1000\n",
            "11/11 - 0s - loss: 9.9562e-04 - val_loss: 0.0085 - 92ms/epoch - 8ms/step\n",
            "Epoch 706/1000\n",
            "11/11 - 0s - loss: 9.9606e-04 - val_loss: 0.0093 - 71ms/epoch - 6ms/step\n",
            "Epoch 707/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 78ms/epoch - 7ms/step\n",
            "Epoch 708/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 104ms/epoch - 9ms/step\n",
            "Epoch 709/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0093 - 84ms/epoch - 8ms/step\n",
            "Epoch 710/1000\n",
            "11/11 - 0s - loss: 9.7979e-04 - val_loss: 0.0080 - 77ms/epoch - 7ms/step\n",
            "Epoch 711/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0100 - 84ms/epoch - 8ms/step\n",
            "Epoch 712/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0089 - 76ms/epoch - 7ms/step\n",
            "Epoch 713/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0079 - 91ms/epoch - 8ms/step\n",
            "Epoch 714/1000\n",
            "11/11 - 0s - loss: 9.9297e-04 - val_loss: 0.0079 - 90ms/epoch - 8ms/step\n",
            "Epoch 715/1000\n",
            "11/11 - 0s - loss: 9.9640e-04 - val_loss: 0.0085 - 90ms/epoch - 8ms/step\n",
            "Epoch 716/1000\n",
            "11/11 - 0s - loss: 9.9770e-04 - val_loss: 0.0082 - 79ms/epoch - 7ms/step\n",
            "Epoch 717/1000\n",
            "11/11 - 0s - loss: 9.8507e-04 - val_loss: 0.0086 - 76ms/epoch - 7ms/step\n",
            "Epoch 718/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 83ms/epoch - 8ms/step\n",
            "Epoch 719/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0075 - 75ms/epoch - 7ms/step\n",
            "Epoch 720/1000\n",
            "11/11 - 0s - loss: 9.7291e-04 - val_loss: 0.0086 - 93ms/epoch - 8ms/step\n",
            "Epoch 721/1000\n",
            "11/11 - 0s - loss: 9.8761e-04 - val_loss: 0.0076 - 74ms/epoch - 7ms/step\n",
            "Epoch 722/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0085 - 87ms/epoch - 8ms/step\n",
            "Epoch 723/1000\n",
            "11/11 - 0s - loss: 9.8339e-04 - val_loss: 0.0081 - 74ms/epoch - 7ms/step\n",
            "Epoch 724/1000\n",
            "11/11 - 0s - loss: 9.9459e-04 - val_loss: 0.0081 - 77ms/epoch - 7ms/step\n",
            "Epoch 725/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0096 - 87ms/epoch - 8ms/step\n",
            "Epoch 726/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0088 - 81ms/epoch - 7ms/step\n",
            "Epoch 727/1000\n",
            "11/11 - 0s - loss: 9.7461e-04 - val_loss: 0.0087 - 75ms/epoch - 7ms/step\n",
            "Epoch 728/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0088 - 81ms/epoch - 7ms/step\n",
            "Epoch 729/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0073 - 84ms/epoch - 8ms/step\n",
            "Epoch 730/1000\n",
            "11/11 - 0s - loss: 9.7493e-04 - val_loss: 0.0090 - 80ms/epoch - 7ms/step\n",
            "Epoch 731/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0067 - 76ms/epoch - 7ms/step\n",
            "Epoch 732/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0079 - 83ms/epoch - 8ms/step\n",
            "Epoch 733/1000\n",
            "11/11 - 0s - loss: 9.6942e-04 - val_loss: 0.0068 - 78ms/epoch - 7ms/step\n",
            "Epoch 734/1000\n",
            "11/11 - 0s - loss: 9.7017e-04 - val_loss: 0.0086 - 84ms/epoch - 8ms/step\n",
            "Epoch 735/1000\n",
            "11/11 - 0s - loss: 9.8097e-04 - val_loss: 0.0085 - 84ms/epoch - 8ms/step\n",
            "Epoch 736/1000\n",
            "11/11 - 0s - loss: 9.9289e-04 - val_loss: 0.0073 - 77ms/epoch - 7ms/step\n",
            "Epoch 737/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 100ms/epoch - 9ms/step\n",
            "Epoch 738/1000\n",
            "11/11 - 0s - loss: 9.7788e-04 - val_loss: 0.0090 - 75ms/epoch - 7ms/step\n",
            "Epoch 739/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0085 - 83ms/epoch - 8ms/step\n",
            "Epoch 740/1000\n",
            "11/11 - 0s - loss: 9.6616e-04 - val_loss: 0.0081 - 105ms/epoch - 10ms/step\n",
            "Epoch 741/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0082 - 78ms/epoch - 7ms/step\n",
            "Epoch 742/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 86ms/epoch - 8ms/step\n",
            "Epoch 743/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0100 - 72ms/epoch - 7ms/step\n",
            "Epoch 744/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0067 - 73ms/epoch - 7ms/step\n",
            "Epoch 745/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0087 - 96ms/epoch - 9ms/step\n",
            "Epoch 746/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 82ms/epoch - 7ms/step\n",
            "Epoch 747/1000\n",
            "11/11 - 0s - loss: 9.9571e-04 - val_loss: 0.0085 - 82ms/epoch - 7ms/step\n",
            "Epoch 748/1000\n",
            "11/11 - 0s - loss: 9.8448e-04 - val_loss: 0.0080 - 87ms/epoch - 8ms/step\n",
            "Epoch 749/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 74ms/epoch - 7ms/step\n",
            "Epoch 750/1000\n",
            "11/11 - 0s - loss: 9.9259e-04 - val_loss: 0.0085 - 81ms/epoch - 7ms/step\n",
            "Epoch 751/1000\n",
            "11/11 - 0s - loss: 9.6440e-04 - val_loss: 0.0077 - 82ms/epoch - 7ms/step\n",
            "Epoch 752/1000\n",
            "11/11 - 0s - loss: 9.7614e-04 - val_loss: 0.0084 - 81ms/epoch - 7ms/step\n",
            "Epoch 753/1000\n",
            "11/11 - 0s - loss: 9.6923e-04 - val_loss: 0.0077 - 82ms/epoch - 7ms/step\n",
            "Epoch 754/1000\n",
            "11/11 - 0s - loss: 9.7538e-04 - val_loss: 0.0077 - 70ms/epoch - 6ms/step\n",
            "Epoch 755/1000\n",
            "11/11 - 0s - loss: 9.9156e-04 - val_loss: 0.0087 - 82ms/epoch - 7ms/step\n",
            "Epoch 756/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0072 - 83ms/epoch - 8ms/step\n",
            "Epoch 757/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0091 - 84ms/epoch - 8ms/step\n",
            "Epoch 758/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0078 - 72ms/epoch - 7ms/step\n",
            "Epoch 759/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0076 - 81ms/epoch - 7ms/step\n",
            "Epoch 760/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0093 - 89ms/epoch - 8ms/step\n",
            "Epoch 761/1000\n",
            "11/11 - 0s - loss: 9.9513e-04 - val_loss: 0.0069 - 80ms/epoch - 7ms/step\n",
            "Epoch 762/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0077 - 83ms/epoch - 8ms/step\n",
            "Epoch 763/1000\n",
            "11/11 - 0s - loss: 9.7166e-04 - val_loss: 0.0086 - 84ms/epoch - 8ms/step\n",
            "Epoch 764/1000\n",
            "11/11 - 0s - loss: 9.7004e-04 - val_loss: 0.0081 - 85ms/epoch - 8ms/step\n",
            "Epoch 765/1000\n",
            "11/11 - 0s - loss: 9.8616e-04 - val_loss: 0.0080 - 89ms/epoch - 8ms/step\n",
            "Epoch 766/1000\n",
            "11/11 - 0s - loss: 9.7818e-04 - val_loss: 0.0080 - 75ms/epoch - 7ms/step\n",
            "Epoch 767/1000\n",
            "11/11 - 0s - loss: 9.7474e-04 - val_loss: 0.0083 - 79ms/epoch - 7ms/step\n",
            "Epoch 768/1000\n",
            "11/11 - 0s - loss: 9.7922e-04 - val_loss: 0.0089 - 85ms/epoch - 8ms/step\n",
            "Epoch 769/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0068 - 89ms/epoch - 8ms/step\n",
            "Epoch 770/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0087 - 82ms/epoch - 7ms/step\n",
            "Epoch 771/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0071 - 89ms/epoch - 8ms/step\n",
            "Epoch 772/1000\n",
            "11/11 - 0s - loss: 9.8542e-04 - val_loss: 0.0080 - 82ms/epoch - 7ms/step\n",
            "Epoch 773/1000\n",
            "11/11 - 0s - loss: 9.9623e-04 - val_loss: 0.0085 - 77ms/epoch - 7ms/step\n",
            "Epoch 774/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 87ms/epoch - 8ms/step\n",
            "Epoch 775/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 776/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0089 - 85ms/epoch - 8ms/step\n",
            "Epoch 777/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0076 - 90ms/epoch - 8ms/step\n",
            "Epoch 778/1000\n",
            "11/11 - 0s - loss: 9.6708e-04 - val_loss: 0.0093 - 97ms/epoch - 9ms/step\n",
            "Epoch 779/1000\n",
            "11/11 - 0s - loss: 9.9758e-04 - val_loss: 0.0078 - 78ms/epoch - 7ms/step\n",
            "Epoch 780/1000\n",
            "11/11 - 0s - loss: 9.8337e-04 - val_loss: 0.0085 - 81ms/epoch - 7ms/step\n",
            "Epoch 781/1000\n",
            "11/11 - 0s - loss: 9.9721e-04 - val_loss: 0.0082 - 84ms/epoch - 8ms/step\n",
            "Epoch 782/1000\n",
            "11/11 - 0s - loss: 9.7326e-04 - val_loss: 0.0076 - 86ms/epoch - 8ms/step\n",
            "Epoch 783/1000\n",
            "11/11 - 0s - loss: 9.9239e-04 - val_loss: 0.0085 - 76ms/epoch - 7ms/step\n",
            "Epoch 784/1000\n",
            "11/11 - 0s - loss: 9.6735e-04 - val_loss: 0.0083 - 92ms/epoch - 8ms/step\n",
            "Epoch 785/1000\n",
            "11/11 - 0s - loss: 9.6704e-04 - val_loss: 0.0077 - 84ms/epoch - 8ms/step\n",
            "Epoch 786/1000\n",
            "11/11 - 0s - loss: 9.9506e-04 - val_loss: 0.0074 - 78ms/epoch - 7ms/step\n",
            "Epoch 787/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0086 - 87ms/epoch - 8ms/step\n",
            "Epoch 788/1000\n",
            "11/11 - 0s - loss: 9.8740e-04 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 789/1000\n",
            "11/11 - 0s - loss: 9.6278e-04 - val_loss: 0.0081 - 73ms/epoch - 7ms/step\n",
            "Epoch 790/1000\n",
            "11/11 - 0s - loss: 9.7644e-04 - val_loss: 0.0085 - 84ms/epoch - 8ms/step\n",
            "Epoch 791/1000\n",
            "11/11 - 0s - loss: 9.5962e-04 - val_loss: 0.0082 - 89ms/epoch - 8ms/step\n",
            "Epoch 792/1000\n",
            "11/11 - 0s - loss: 9.6755e-04 - val_loss: 0.0075 - 82ms/epoch - 7ms/step\n",
            "Epoch 793/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 76ms/epoch - 7ms/step\n",
            "Epoch 794/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0081 - 108ms/epoch - 10ms/step\n",
            "Epoch 795/1000\n",
            "11/11 - 0s - loss: 9.6417e-04 - val_loss: 0.0077 - 82ms/epoch - 7ms/step\n",
            "Epoch 796/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0089 - 76ms/epoch - 7ms/step\n",
            "Epoch 797/1000\n",
            "11/11 - 0s - loss: 9.7034e-04 - val_loss: 0.0074 - 83ms/epoch - 8ms/step\n",
            "Epoch 798/1000\n",
            "11/11 - 0s - loss: 9.8346e-04 - val_loss: 0.0077 - 80ms/epoch - 7ms/step\n",
            "Epoch 799/1000\n",
            "11/11 - 0s - loss: 9.7853e-04 - val_loss: 0.0080 - 80ms/epoch - 7ms/step\n",
            "Epoch 800/1000\n",
            "11/11 - 0s - loss: 9.7710e-04 - val_loss: 0.0081 - 80ms/epoch - 7ms/step\n",
            "Epoch 801/1000\n",
            "11/11 - 0s - loss: 9.8059e-04 - val_loss: 0.0078 - 77ms/epoch - 7ms/step\n",
            "Epoch 802/1000\n",
            "11/11 - 0s - loss: 9.5904e-04 - val_loss: 0.0097 - 86ms/epoch - 8ms/step\n",
            "Epoch 803/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0059 - 91ms/epoch - 8ms/step\n",
            "Epoch 804/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0080 - 85ms/epoch - 8ms/step\n",
            "Epoch 805/1000\n",
            "11/11 - 0s - loss: 9.8580e-04 - val_loss: 0.0093 - 87ms/epoch - 8ms/step\n",
            "Epoch 806/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0078 - 80ms/epoch - 7ms/step\n",
            "Epoch 807/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 81ms/epoch - 7ms/step\n",
            "Epoch 808/1000\n",
            "11/11 - 0s - loss: 9.9493e-04 - val_loss: 0.0077 - 79ms/epoch - 7ms/step\n",
            "Epoch 809/1000\n",
            "11/11 - 0s - loss: 9.6211e-04 - val_loss: 0.0077 - 77ms/epoch - 7ms/step\n",
            "Epoch 810/1000\n",
            "11/11 - 0s - loss: 9.6582e-04 - val_loss: 0.0079 - 89ms/epoch - 8ms/step\n",
            "Epoch 811/1000\n",
            "11/11 - 0s - loss: 9.7023e-04 - val_loss: 0.0081 - 76ms/epoch - 7ms/step\n",
            "Epoch 812/1000\n",
            "11/11 - 0s - loss: 9.6668e-04 - val_loss: 0.0089 - 75ms/epoch - 7ms/step\n",
            "Epoch 813/1000\n",
            "11/11 - 0s - loss: 9.5158e-04 - val_loss: 0.0071 - 80ms/epoch - 7ms/step\n",
            "Epoch 814/1000\n",
            "11/11 - 0s - loss: 9.8115e-04 - val_loss: 0.0091 - 83ms/epoch - 8ms/step\n",
            "Epoch 815/1000\n",
            "11/11 - 0s - loss: 9.8101e-04 - val_loss: 0.0076 - 80ms/epoch - 7ms/step\n",
            "Epoch 816/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0074 - 79ms/epoch - 7ms/step\n",
            "Epoch 817/1000\n",
            "11/11 - 0s - loss: 9.8709e-04 - val_loss: 0.0073 - 85ms/epoch - 8ms/step\n",
            "Epoch 818/1000\n",
            "11/11 - 0s - loss: 9.6471e-04 - val_loss: 0.0075 - 76ms/epoch - 7ms/step\n",
            "Epoch 819/1000\n",
            "11/11 - 0s - loss: 9.5769e-04 - val_loss: 0.0077 - 76ms/epoch - 7ms/step\n",
            "Epoch 820/1000\n",
            "11/11 - 0s - loss: 9.6198e-04 - val_loss: 0.0081 - 85ms/epoch - 8ms/step\n",
            "Epoch 821/1000\n",
            "11/11 - 0s - loss: 9.6128e-04 - val_loss: 0.0073 - 83ms/epoch - 8ms/step\n",
            "Epoch 822/1000\n",
            "11/11 - 0s - loss: 9.5440e-04 - val_loss: 0.0085 - 80ms/epoch - 7ms/step\n",
            "Epoch 823/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 79ms/epoch - 7ms/step\n",
            "Epoch 824/1000\n",
            "11/11 - 0s - loss: 9.7200e-04 - val_loss: 0.0080 - 79ms/epoch - 7ms/step\n",
            "Epoch 825/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 78ms/epoch - 7ms/step\n",
            "Epoch 826/1000\n",
            "11/11 - 0s - loss: 9.8592e-04 - val_loss: 0.0079 - 92ms/epoch - 8ms/step\n",
            "Epoch 827/1000\n",
            "11/11 - 0s - loss: 9.6126e-04 - val_loss: 0.0085 - 82ms/epoch - 7ms/step\n",
            "Epoch 828/1000\n",
            "11/11 - 0s - loss: 9.8046e-04 - val_loss: 0.0087 - 94ms/epoch - 9ms/step\n",
            "Epoch 829/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 83ms/epoch - 8ms/step\n",
            "Epoch 830/1000\n",
            "11/11 - 0s - loss: 9.9096e-04 - val_loss: 0.0073 - 86ms/epoch - 8ms/step\n",
            "Epoch 831/1000\n",
            "11/11 - 0s - loss: 9.9955e-04 - val_loss: 0.0076 - 78ms/epoch - 7ms/step\n",
            "Epoch 832/1000\n",
            "11/11 - 0s - loss: 9.7404e-04 - val_loss: 0.0068 - 80ms/epoch - 7ms/step\n",
            "Epoch 833/1000\n",
            "11/11 - 0s - loss: 9.6452e-04 - val_loss: 0.0088 - 92ms/epoch - 8ms/step\n",
            "Epoch 834/1000\n",
            "11/11 - 0s - loss: 9.8955e-04 - val_loss: 0.0062 - 79ms/epoch - 7ms/step\n",
            "Epoch 835/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0102 - 89ms/epoch - 8ms/step\n",
            "Epoch 836/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0065 - 90ms/epoch - 8ms/step\n",
            "Epoch 837/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 80ms/epoch - 7ms/step\n",
            "Epoch 838/1000\n",
            "11/11 - 0s - loss: 9.9614e-04 - val_loss: 0.0074 - 77ms/epoch - 7ms/step\n",
            "Epoch 839/1000\n",
            "11/11 - 0s - loss: 9.9123e-04 - val_loss: 0.0068 - 80ms/epoch - 7ms/step\n",
            "Epoch 840/1000\n",
            "11/11 - 0s - loss: 9.9089e-04 - val_loss: 0.0083 - 82ms/epoch - 7ms/step\n",
            "Epoch 841/1000\n",
            "11/11 - 0s - loss: 9.4928e-04 - val_loss: 0.0070 - 84ms/epoch - 8ms/step\n",
            "Epoch 842/1000\n",
            "11/11 - 0s - loss: 9.7750e-04 - val_loss: 0.0083 - 78ms/epoch - 7ms/step\n",
            "Epoch 843/1000\n",
            "11/11 - 0s - loss: 9.8770e-04 - val_loss: 0.0070 - 79ms/epoch - 7ms/step\n",
            "Epoch 844/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0098 - 77ms/epoch - 7ms/step\n",
            "Epoch 845/1000\n",
            "11/11 - 0s - loss: 0.0015 - val_loss: 0.0065 - 72ms/epoch - 7ms/step\n",
            "Epoch 846/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0073 - 73ms/epoch - 7ms/step\n",
            "Epoch 847/1000\n",
            "11/11 - 0s - loss: 9.6700e-04 - val_loss: 0.0090 - 77ms/epoch - 7ms/step\n",
            "Epoch 848/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0090 - 83ms/epoch - 8ms/step\n",
            "Epoch 849/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0086 - 78ms/epoch - 7ms/step\n",
            "Epoch 850/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0078 - 76ms/epoch - 7ms/step\n",
            "Epoch 851/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0085 - 100ms/epoch - 9ms/step\n",
            "Epoch 852/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0073 - 97ms/epoch - 9ms/step\n",
            "Epoch 853/1000\n",
            "11/11 - 0s - loss: 9.9927e-04 - val_loss: 0.0098 - 79ms/epoch - 7ms/step\n",
            "Epoch 854/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0068 - 86ms/epoch - 8ms/step\n",
            "Epoch 855/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0077 - 85ms/epoch - 8ms/step\n",
            "Epoch 856/1000\n",
            "11/11 - 0s - loss: 9.6436e-04 - val_loss: 0.0077 - 79ms/epoch - 7ms/step\n",
            "Epoch 857/1000\n",
            "11/11 - 0s - loss: 9.8122e-04 - val_loss: 0.0090 - 91ms/epoch - 8ms/step\n",
            "Epoch 858/1000\n",
            "11/11 - 0s - loss: 9.9309e-04 - val_loss: 0.0084 - 77ms/epoch - 7ms/step\n",
            "Epoch 859/1000\n",
            "11/11 - 0s - loss: 9.9846e-04 - val_loss: 0.0072 - 71ms/epoch - 6ms/step\n",
            "Epoch 860/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0085 - 85ms/epoch - 8ms/step\n",
            "Epoch 861/1000\n",
            "11/11 - 0s - loss: 9.9472e-04 - val_loss: 0.0079 - 81ms/epoch - 7ms/step\n",
            "Epoch 862/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0082 - 90ms/epoch - 8ms/step\n",
            "Epoch 863/1000\n",
            "11/11 - 0s - loss: 9.8424e-04 - val_loss: 0.0078 - 95ms/epoch - 9ms/step\n",
            "Epoch 864/1000\n",
            "11/11 - 0s - loss: 9.6950e-04 - val_loss: 0.0085 - 86ms/epoch - 8ms/step\n",
            "Epoch 865/1000\n",
            "11/11 - 0s - loss: 9.6207e-04 - val_loss: 0.0096 - 74ms/epoch - 7ms/step\n",
            "Epoch 866/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0078 - 85ms/epoch - 8ms/step\n",
            "Epoch 867/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0074 - 92ms/epoch - 8ms/step\n",
            "Epoch 868/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0083 - 79ms/epoch - 7ms/step\n",
            "Epoch 869/1000\n",
            "11/11 - 0s - loss: 9.5043e-04 - val_loss: 0.0068 - 79ms/epoch - 7ms/step\n",
            "Epoch 870/1000\n",
            "11/11 - 0s - loss: 9.8985e-04 - val_loss: 0.0082 - 76ms/epoch - 7ms/step\n",
            "Epoch 871/1000\n",
            "11/11 - 0s - loss: 9.6641e-04 - val_loss: 0.0067 - 78ms/epoch - 7ms/step\n",
            "Epoch 872/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0082 - 81ms/epoch - 7ms/step\n",
            "Epoch 873/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0084 - 99ms/epoch - 9ms/step\n",
            "Epoch 874/1000\n",
            "11/11 - 0s - loss: 9.5390e-04 - val_loss: 0.0076 - 96ms/epoch - 9ms/step\n",
            "Epoch 875/1000\n",
            "11/11 - 0s - loss: 9.6330e-04 - val_loss: 0.0074 - 80ms/epoch - 7ms/step\n",
            "Epoch 876/1000\n",
            "11/11 - 0s - loss: 9.6955e-04 - val_loss: 0.0086 - 89ms/epoch - 8ms/step\n",
            "Epoch 877/1000\n",
            "11/11 - 0s - loss: 9.9433e-04 - val_loss: 0.0074 - 83ms/epoch - 8ms/step\n",
            "Epoch 878/1000\n",
            "11/11 - 0s - loss: 9.6041e-04 - val_loss: 0.0080 - 95ms/epoch - 9ms/step\n",
            "Epoch 879/1000\n",
            "11/11 - 0s - loss: 9.9003e-04 - val_loss: 0.0078 - 80ms/epoch - 7ms/step\n",
            "Epoch 880/1000\n",
            "11/11 - 0s - loss: 9.6611e-04 - val_loss: 0.0072 - 101ms/epoch - 9ms/step\n",
            "Epoch 881/1000\n",
            "11/11 - 0s - loss: 9.7048e-04 - val_loss: 0.0066 - 87ms/epoch - 8ms/step\n",
            "Epoch 882/1000\n",
            "11/11 - 0s - loss: 9.7569e-04 - val_loss: 0.0071 - 75ms/epoch - 7ms/step\n",
            "Epoch 883/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0074 - 87ms/epoch - 8ms/step\n",
            "Epoch 884/1000\n",
            "11/11 - 0s - loss: 9.9437e-04 - val_loss: 0.0077 - 84ms/epoch - 8ms/step\n",
            "Epoch 885/1000\n",
            "11/11 - 0s - loss: 9.7743e-04 - val_loss: 0.0063 - 90ms/epoch - 8ms/step\n",
            "Epoch 886/1000\n",
            "11/11 - 0s - loss: 9.6381e-04 - val_loss: 0.0088 - 89ms/epoch - 8ms/step\n",
            "Epoch 887/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0074 - 94ms/epoch - 9ms/step\n",
            "Epoch 888/1000\n",
            "11/11 - 0s - loss: 9.7927e-04 - val_loss: 0.0084 - 82ms/epoch - 7ms/step\n",
            "Epoch 889/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0092 - 77ms/epoch - 7ms/step\n",
            "Epoch 890/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0083 - 83ms/epoch - 8ms/step\n",
            "Epoch 891/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0075 - 88ms/epoch - 8ms/step\n",
            "Epoch 892/1000\n",
            "11/11 - 0s - loss: 9.5494e-04 - val_loss: 0.0085 - 83ms/epoch - 8ms/step\n",
            "Epoch 893/1000\n",
            "11/11 - 0s - loss: 9.6554e-04 - val_loss: 0.0076 - 88ms/epoch - 8ms/step\n",
            "Epoch 894/1000\n",
            "11/11 - 0s - loss: 9.7959e-04 - val_loss: 0.0071 - 86ms/epoch - 8ms/step\n",
            "Epoch 895/1000\n",
            "11/11 - 0s - loss: 9.9793e-04 - val_loss: 0.0083 - 83ms/epoch - 8ms/step\n",
            "Epoch 896/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0070 - 99ms/epoch - 9ms/step\n",
            "Epoch 897/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 80ms/epoch - 7ms/step\n",
            "Epoch 898/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0073 - 82ms/epoch - 7ms/step\n",
            "Epoch 899/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0074 - 82ms/epoch - 7ms/step\n",
            "Epoch 900/1000\n",
            "11/11 - 0s - loss: 9.6084e-04 - val_loss: 0.0074 - 86ms/epoch - 8ms/step\n",
            "Epoch 901/1000\n",
            "11/11 - 0s - loss: 9.4633e-04 - val_loss: 0.0073 - 80ms/epoch - 7ms/step\n",
            "Epoch 902/1000\n",
            "11/11 - 0s - loss: 9.5850e-04 - val_loss: 0.0073 - 80ms/epoch - 7ms/step\n",
            "Epoch 903/1000\n",
            "11/11 - 0s - loss: 9.5021e-04 - val_loss: 0.0074 - 84ms/epoch - 8ms/step\n",
            "Epoch 904/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 86ms/epoch - 8ms/step\n",
            "Epoch 905/1000\n",
            "11/11 - 0s - loss: 9.4850e-04 - val_loss: 0.0074 - 87ms/epoch - 8ms/step\n",
            "Epoch 906/1000\n",
            "11/11 - 0s - loss: 9.5940e-04 - val_loss: 0.0070 - 81ms/epoch - 7ms/step\n",
            "Epoch 907/1000\n",
            "11/11 - 0s - loss: 9.4216e-04 - val_loss: 0.0076 - 93ms/epoch - 8ms/step\n",
            "Epoch 908/1000\n",
            "11/11 - 0s - loss: 9.4666e-04 - val_loss: 0.0076 - 88ms/epoch - 8ms/step\n",
            "Epoch 909/1000\n",
            "11/11 - 0s - loss: 9.4998e-04 - val_loss: 0.0079 - 83ms/epoch - 8ms/step\n",
            "Epoch 910/1000\n",
            "11/11 - 0s - loss: 9.5692e-04 - val_loss: 0.0075 - 84ms/epoch - 8ms/step\n",
            "Epoch 911/1000\n",
            "11/11 - 0s - loss: 9.6802e-04 - val_loss: 0.0069 - 80ms/epoch - 7ms/step\n",
            "Epoch 912/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0081 - 81ms/epoch - 7ms/step\n",
            "Epoch 913/1000\n",
            "11/11 - 0s - loss: 9.6929e-04 - val_loss: 0.0068 - 81ms/epoch - 7ms/step\n",
            "Epoch 914/1000\n",
            "11/11 - 0s - loss: 0.0012 - val_loss: 0.0073 - 77ms/epoch - 7ms/step\n",
            "Epoch 915/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0071 - 82ms/epoch - 7ms/step\n",
            "Epoch 916/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0073 - 85ms/epoch - 8ms/step\n",
            "Epoch 917/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0081 - 75ms/epoch - 7ms/step\n",
            "Epoch 918/1000\n",
            "11/11 - 0s - loss: 9.4625e-04 - val_loss: 0.0070 - 87ms/epoch - 8ms/step\n",
            "Epoch 919/1000\n",
            "11/11 - 0s - loss: 9.6663e-04 - val_loss: 0.0070 - 88ms/epoch - 8ms/step\n",
            "Epoch 920/1000\n",
            "11/11 - 0s - loss: 9.4443e-04 - val_loss: 0.0082 - 84ms/epoch - 8ms/step\n",
            "Epoch 921/1000\n",
            "11/11 - 0s - loss: 9.9114e-04 - val_loss: 0.0077 - 85ms/epoch - 8ms/step\n",
            "Epoch 922/1000\n",
            "11/11 - 0s - loss: 9.5021e-04 - val_loss: 0.0072 - 84ms/epoch - 8ms/step\n",
            "Epoch 923/1000\n",
            "11/11 - 0s - loss: 9.4354e-04 - val_loss: 0.0061 - 80ms/epoch - 7ms/step\n",
            "Epoch 924/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0074 - 86ms/epoch - 8ms/step\n",
            "Epoch 925/1000\n",
            "11/11 - 0s - loss: 9.4582e-04 - val_loss: 0.0079 - 78ms/epoch - 7ms/step\n",
            "Epoch 926/1000\n",
            "11/11 - 0s - loss: 9.4784e-04 - val_loss: 0.0073 - 87ms/epoch - 8ms/step\n",
            "Epoch 927/1000\n",
            "11/11 - 0s - loss: 9.6674e-04 - val_loss: 0.0080 - 78ms/epoch - 7ms/step\n",
            "Epoch 928/1000\n",
            "11/11 - 0s - loss: 9.7444e-04 - val_loss: 0.0081 - 78ms/epoch - 7ms/step\n",
            "Epoch 929/1000\n",
            "11/11 - 0s - loss: 9.5373e-04 - val_loss: 0.0068 - 94ms/epoch - 9ms/step\n",
            "Epoch 930/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0075 - 92ms/epoch - 8ms/step\n",
            "Epoch 931/1000\n",
            "11/11 - 0s - loss: 9.5109e-04 - val_loss: 0.0084 - 80ms/epoch - 7ms/step\n",
            "Epoch 932/1000\n",
            "11/11 - 0s - loss: 9.4534e-04 - val_loss: 0.0074 - 81ms/epoch - 7ms/step\n",
            "Epoch 933/1000\n",
            "11/11 - 0s - loss: 9.4243e-04 - val_loss: 0.0080 - 82ms/epoch - 7ms/step\n",
            "Epoch 934/1000\n",
            "11/11 - 0s - loss: 9.6315e-04 - val_loss: 0.0084 - 82ms/epoch - 7ms/step\n",
            "Epoch 935/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0069 - 81ms/epoch - 7ms/step\n",
            "Epoch 936/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0070 - 79ms/epoch - 7ms/step\n",
            "Epoch 937/1000\n",
            "11/11 - 0s - loss: 9.3424e-04 - val_loss: 0.0092 - 105ms/epoch - 10ms/step\n",
            "Epoch 938/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0067 - 71ms/epoch - 6ms/step\n",
            "Epoch 939/1000\n",
            "11/11 - 0s - loss: 9.9399e-04 - val_loss: 0.0080 - 77ms/epoch - 7ms/step\n",
            "Epoch 940/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0060 - 91ms/epoch - 8ms/step\n",
            "Epoch 941/1000\n",
            "11/11 - 0s - loss: 9.9138e-04 - val_loss: 0.0080 - 89ms/epoch - 8ms/step\n",
            "Epoch 942/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 79ms/epoch - 7ms/step\n",
            "Epoch 943/1000\n",
            "11/11 - 0s - loss: 9.5197e-04 - val_loss: 0.0071 - 84ms/epoch - 8ms/step\n",
            "Epoch 944/1000\n",
            "11/11 - 0s - loss: 9.5846e-04 - val_loss: 0.0081 - 87ms/epoch - 8ms/step\n",
            "Epoch 945/1000\n",
            "11/11 - 0s - loss: 9.7658e-04 - val_loss: 0.0067 - 86ms/epoch - 8ms/step\n",
            "Epoch 946/1000\n",
            "11/11 - 0s - loss: 9.6296e-04 - val_loss: 0.0074 - 79ms/epoch - 7ms/step\n",
            "Epoch 947/1000\n",
            "11/11 - 0s - loss: 9.9296e-04 - val_loss: 0.0081 - 90ms/epoch - 8ms/step\n",
            "Epoch 948/1000\n",
            "11/11 - 0s - loss: 9.4606e-04 - val_loss: 0.0069 - 82ms/epoch - 7ms/step\n",
            "Epoch 949/1000\n",
            "11/11 - 0s - loss: 9.6595e-04 - val_loss: 0.0085 - 82ms/epoch - 7ms/step\n",
            "Epoch 950/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0069 - 85ms/epoch - 8ms/step\n",
            "Epoch 951/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0072 - 74ms/epoch - 7ms/step\n",
            "Epoch 952/1000\n",
            "11/11 - 0s - loss: 9.5959e-04 - val_loss: 0.0066 - 89ms/epoch - 8ms/step\n",
            "Epoch 953/1000\n",
            "11/11 - 0s - loss: 9.6657e-04 - val_loss: 0.0086 - 81ms/epoch - 7ms/step\n",
            "Epoch 954/1000\n",
            "11/11 - 0s - loss: 9.6642e-04 - val_loss: 0.0081 - 94ms/epoch - 9ms/step\n",
            "Epoch 955/1000\n",
            "11/11 - 0s - loss: 9.6070e-04 - val_loss: 0.0078 - 85ms/epoch - 8ms/step\n",
            "Epoch 956/1000\n",
            "11/11 - 0s - loss: 9.4114e-04 - val_loss: 0.0081 - 85ms/epoch - 8ms/step\n",
            "Epoch 957/1000\n",
            "11/11 - 0s - loss: 9.5426e-04 - val_loss: 0.0066 - 74ms/epoch - 7ms/step\n",
            "Epoch 958/1000\n",
            "11/11 - 0s - loss: 9.5566e-04 - val_loss: 0.0095 - 77ms/epoch - 7ms/step\n",
            "Epoch 959/1000\n",
            "11/11 - 0s - loss: 0.0014 - val_loss: 0.0063 - 92ms/epoch - 8ms/step\n",
            "Epoch 960/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0088 - 80ms/epoch - 7ms/step\n",
            "Epoch 961/1000\n",
            "11/11 - 0s - loss: 0.0013 - val_loss: 0.0073 - 84ms/epoch - 8ms/step\n",
            "Epoch 962/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0061 - 88ms/epoch - 8ms/step\n",
            "Epoch 963/1000\n",
            "11/11 - 0s - loss: 9.5761e-04 - val_loss: 0.0089 - 91ms/epoch - 8ms/step\n",
            "Epoch 964/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0074 - 95ms/epoch - 9ms/step\n",
            "Epoch 965/1000\n",
            "11/11 - 0s - loss: 9.9799e-04 - val_loss: 0.0077 - 86ms/epoch - 8ms/step\n",
            "Epoch 966/1000\n",
            "11/11 - 0s - loss: 9.4722e-04 - val_loss: 0.0080 - 85ms/epoch - 8ms/step\n",
            "Epoch 967/1000\n",
            "11/11 - 0s - loss: 9.9045e-04 - val_loss: 0.0069 - 84ms/epoch - 8ms/step\n",
            "Epoch 968/1000\n",
            "11/11 - 0s - loss: 9.8109e-04 - val_loss: 0.0078 - 81ms/epoch - 7ms/step\n",
            "Epoch 969/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0075 - 84ms/epoch - 8ms/step\n",
            "Epoch 970/1000\n",
            "11/11 - 0s - loss: 9.9489e-04 - val_loss: 0.0069 - 85ms/epoch - 8ms/step\n",
            "Epoch 971/1000\n",
            "11/11 - 0s - loss: 9.5633e-04 - val_loss: 0.0081 - 78ms/epoch - 7ms/step\n",
            "Epoch 972/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0070 - 99ms/epoch - 9ms/step\n",
            "Epoch 973/1000\n",
            "11/11 - 0s - loss: 9.6646e-04 - val_loss: 0.0069 - 88ms/epoch - 8ms/step\n",
            "Epoch 974/1000\n",
            "11/11 - 0s - loss: 9.7739e-04 - val_loss: 0.0077 - 89ms/epoch - 8ms/step\n",
            "Epoch 975/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0070 - 91ms/epoch - 8ms/step\n",
            "Epoch 976/1000\n",
            "11/11 - 0s - loss: 9.5711e-04 - val_loss: 0.0060 - 73ms/epoch - 7ms/step\n",
            "Epoch 977/1000\n",
            "11/11 - 0s - loss: 9.6627e-04 - val_loss: 0.0083 - 80ms/epoch - 7ms/step\n",
            "Epoch 978/1000\n",
            "11/11 - 0s - loss: 9.6789e-04 - val_loss: 0.0073 - 83ms/epoch - 8ms/step\n",
            "Epoch 979/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0077 - 83ms/epoch - 8ms/step\n",
            "Epoch 980/1000\n",
            "11/11 - 0s - loss: 9.4799e-04 - val_loss: 0.0082 - 101ms/epoch - 9ms/step\n",
            "Epoch 981/1000\n",
            "11/11 - 0s - loss: 9.4507e-04 - val_loss: 0.0075 - 85ms/epoch - 8ms/step\n",
            "Epoch 982/1000\n",
            "11/11 - 0s - loss: 9.5236e-04 - val_loss: 0.0073 - 91ms/epoch - 8ms/step\n",
            "Epoch 983/1000\n",
            "11/11 - 0s - loss: 9.4131e-04 - val_loss: 0.0067 - 83ms/epoch - 8ms/step\n",
            "Epoch 984/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0080 - 84ms/epoch - 8ms/step\n",
            "Epoch 985/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0070 - 88ms/epoch - 8ms/step\n",
            "Epoch 986/1000\n",
            "11/11 - 0s - loss: 9.8261e-04 - val_loss: 0.0068 - 89ms/epoch - 8ms/step\n",
            "Epoch 987/1000\n",
            "11/11 - 0s - loss: 9.4148e-04 - val_loss: 0.0077 - 92ms/epoch - 8ms/step\n",
            "Epoch 988/1000\n",
            "11/11 - 0s - loss: 9.8310e-04 - val_loss: 0.0069 - 84ms/epoch - 8ms/step\n",
            "Epoch 989/1000\n",
            "11/11 - 0s - loss: 9.7086e-04 - val_loss: 0.0073 - 85ms/epoch - 8ms/step\n",
            "Epoch 990/1000\n",
            "11/11 - 0s - loss: 9.4246e-04 - val_loss: 0.0068 - 84ms/epoch - 8ms/step\n",
            "Epoch 991/1000\n",
            "11/11 - 0s - loss: 9.4609e-04 - val_loss: 0.0070 - 78ms/epoch - 7ms/step\n",
            "Epoch 992/1000\n",
            "11/11 - 0s - loss: 9.6748e-04 - val_loss: 0.0078 - 84ms/epoch - 8ms/step\n",
            "Epoch 993/1000\n",
            "11/11 - 0s - loss: 9.4965e-04 - val_loss: 0.0081 - 87ms/epoch - 8ms/step\n",
            "Epoch 994/1000\n",
            "11/11 - 0s - loss: 0.0010 - val_loss: 0.0060 - 77ms/epoch - 7ms/step\n",
            "Epoch 995/1000\n",
            "11/11 - 0s - loss: 9.7032e-04 - val_loss: 0.0081 - 81ms/epoch - 7ms/step\n",
            "Epoch 996/1000\n",
            "11/11 - 0s - loss: 0.0011 - val_loss: 0.0065 - 86ms/epoch - 8ms/step\n",
            "Epoch 997/1000\n",
            "11/11 - 0s - loss: 9.9928e-04 - val_loss: 0.0067 - 99ms/epoch - 9ms/step\n",
            "Epoch 998/1000\n",
            "11/11 - 0s - loss: 9.4567e-04 - val_loss: 0.0085 - 89ms/epoch - 8ms/step\n",
            "Epoch 999/1000\n",
            "11/11 - 0s - loss: 9.5828e-04 - val_loss: 0.0069 - 78ms/epoch - 7ms/step\n",
            "Epoch 1000/1000\n",
            "11/11 - 0s - loss: 9.5735e-04 - val_loss: 0.0064 - 80ms/epoch - 7ms/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 4)                 68        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance (10 Points)\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. **(2 Points)**\n",
        "2. Scale the prediction results back to original representation with the scaler.(scaler.inverse_transform function) **(3 Points)**\n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**. **(3 Points)**\n",
        "4. **Plot** the **TEST** label and prediction. **(2 Points)**\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        "2. For validation: Train Score: **~10.92 RMSE** Test Score: **~27.70 RMSE**\n",
        "3. The plot for validation is shown below (observation test data are blue and prediction results are orange):\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10c1Fsa_9v0AQf2fDpCzGPFPxbIEso81u)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEkAMxnz8Mq"
      },
      "source": [
        "### Make Predictions ###\n",
        "\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "source": [
        "### Scale Back Predictions ###\n",
        "\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform([trainY])  # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform([testY]) # scale test labels back with scaler.inverse_transform()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67e0d06-80ee-4535-b65c-cbca7c19516b"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0])) \n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score: 11.18 RMSE\n",
            "Test Score: 28.98 RMSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "61a6c6f3-d060-480a-b9e9-799717d8e555"
      },
      "source": [
        "### Plot Observation Data and Prediction Results with TEST dataset ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(testY[0]) # Plot Observations in Test Set\n",
        "plt.plot(testPredict) # Plot Predictions in Test Set\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data (13 Points) ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences. **(3 Points)**\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. **(10 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features). Here. max_features is set to **1000**.\n",
        "2. You may use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvV1Sv2a18SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5202b282-9b45-4ef8-80ab-517f8e2c445f"
      },
      "source": [
        "# Prepare the data here\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data( ) # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) # Pad IMDB training data with specified maxlen=100\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen) # Pad IMDB validation data with specified maxlen=100\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model (25 Points) ###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**. **(10 Points)**\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics. **(5 Points)**  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. **(5 Points)**\n",
        "4. **Print out** best validation accuracy. **(5 Points)**\n",
        "\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Set input dimension to **1000** and output dimension to **8** for embedding layer.\n",
        "2. Set **unit_size=8** for LSTM layer.\n",
        "3. Set activation function to **sigmoid** for Dense layer.\n",
        "4. For validation: the outputs for first epoch should be close to（but maybe not exactly following） the statistics below:\\\n",
        "- **-loss: ~0.6402 - accuracy: ~0.6187 - val_loss: ~0.4645 - val_accuracy: ~0.7995**\n",
        "5. The model summary is as follows:\n",
        "- Total params: 8,553\n",
        "- Trainable params: 8,553\n",
        "- Non-trainable params: 0\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqqgFt118SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c918ee7-e050-4c1b-e7a3-909892f0010d"
      },
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 8)(inputs) # Embed data in an 8-dimensional vector\n",
        "x = layers.LSTM(8)(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model with model.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 8)           8000      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 8)                 544       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,553\n",
            "Trainable params: 8,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 15s 28ms/step - loss: 0.5597 - accuracy: 0.7118 - val_loss: 0.4440 - val_accuracy: 0.8022\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 0.4100 - accuracy: 0.8181 - val_loss: 0.3972 - val_accuracy: 0.8218\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3811 - accuracy: 0.8328 - val_loss: 0.3846 - val_accuracy: 0.8283\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 0.3673 - accuracy: 0.8398 - val_loss: 0.3774 - val_accuracy: 0.8292\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 0.3611 - accuracy: 0.8437 - val_loss: 0.3790 - val_accuracy: 0.8290\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 11s 27ms/step - loss: 0.3574 - accuracy: 0.8443 - val_loss: 0.4148 - val_accuracy: 0.8212\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 0.3512 - accuracy: 0.8444 - val_loss: 0.3712 - val_accuracy: 0.8323\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 13s 32ms/step - loss: 0.3441 - accuracy: 0.8478 - val_loss: 0.3731 - val_accuracy: 0.8300\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3385 - accuracy: 0.8490 - val_loss: 0.3903 - val_accuracy: 0.8308\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3330 - accuracy: 0.8534 - val_loss: 0.3809 - val_accuracy: 0.8290\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc911d56610>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning (Bonus 15 Points) ###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "Note: \n",
        "- All modificiations are directly conducted based on the vanilla model above (from 2.2).\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. You may just paste the plot figures in the cells with **Markdown mode**, or leave the result after running. **Make sure it is already correctly shown in your submitted file.**\n",
        "1.  Scenario 1 (**5 points**):\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "**Hints:**  \n",
        "For validation: the outputs for first epoch should be close to （but maybe not exactly following） the statistics below:\n",
        "- Scenario 1: **loss: ~0.5839 - accuracy: ~0.6524 - val_loss: ~0.4079 - val_accuracy: ~0.8198**\n",
        "- Scenario 2: **loss: ~0.5572 - accuracy: ~0.6911 - val_loss: ~0.3953 - val_accuracy: ~0.8244**\n",
        "- Scenario 3: **loss: ~0.5605 - accuracy: ~0.6914 - val_loss: ~0.3402 - val_accuracy: ~0.8560**\n",
        "\n",
        "- Summary of Model 1: Total params: 20,241; Trainable params: 20,241; Non-trainable params: 0\n",
        "- Summary of Model 2: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "- Summary of Model 3: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "\n",
        "You may follow the example from the reference below to add additional LSTM layer.\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xMSM_GQt_P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269afab0-b91b-4a53-ce6d-75dce78f6768"
      },
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 16)(inputs) # Embed data in a 16-dimensional vector\n",
        "x = layers.LSTM(16, return_sequences=True)(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(16)(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 16)          16000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 16)          2112      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                2112      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,241\n",
            "Trainable params: 20,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 25s 54ms/step - loss: 0.4909 - accuracy: 0.7496 - val_loss: 0.4070 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3877 - accuracy: 0.8287 - val_loss: 0.3919 - val_accuracy: 0.8193\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3708 - accuracy: 0.8357 - val_loss: 0.3714 - val_accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3583 - accuracy: 0.8411 - val_loss: 0.3753 - val_accuracy: 0.8274\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3491 - accuracy: 0.8442 - val_loss: 0.3707 - val_accuracy: 0.8320\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.3402 - accuracy: 0.8491 - val_loss: 0.4193 - val_accuracy: 0.8199\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3306 - accuracy: 0.8508 - val_loss: 0.3687 - val_accuracy: 0.8324\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3235 - accuracy: 0.8577 - val_loss: 0.3686 - val_accuracy: 0.8333\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3157 - accuracy: 0.8574 - val_loss: 0.3768 - val_accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3086 - accuracy: 0.8648 - val_loss: 0.3969 - val_accuracy: 0.8329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc91142b290>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keod5xXkEKnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257e6826-7ef5-4812-c84a-d3845395251f"
      },
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 29s 64ms/step - loss: 0.4616 - accuracy: 0.7769 - val_loss: 0.3900 - val_accuracy: 0.8262\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.3775 - accuracy: 0.8339 - val_loss: 0.3806 - val_accuracy: 0.8250\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3552 - accuracy: 0.8423 - val_loss: 0.3670 - val_accuracy: 0.8338\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3321 - accuracy: 0.8550 - val_loss: 0.3667 - val_accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.3142 - accuracy: 0.8657 - val_loss: 0.3610 - val_accuracy: 0.8415\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2941 - accuracy: 0.8774 - val_loss: 0.3990 - val_accuracy: 0.8254\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2823 - accuracy: 0.8804 - val_loss: 0.3799 - val_accuracy: 0.8367\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.2676 - accuracy: 0.8890 - val_loss: 0.3674 - val_accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2499 - accuracy: 0.8974 - val_loss: 0.3912 - val_accuracy: 0.8241\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2329 - accuracy: 0.9060 - val_loss: 0.4016 - val_accuracy: 0.8309\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc90e7b3ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdiZbuCQt_QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc8e6fb-9b8a-4cf6-ef4e-ecc111b2fa57"
      },
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 200 # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 27s 60ms/step - loss: 0.4616 - accuracy: 0.7769 - val_loss: 0.3900 - val_accuracy: 0.8262\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3775 - accuracy: 0.8339 - val_loss: 0.3806 - val_accuracy: 0.8251\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3554 - accuracy: 0.8432 - val_loss: 0.3655 - val_accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.3325 - accuracy: 0.8546 - val_loss: 0.3629 - val_accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3144 - accuracy: 0.8636 - val_loss: 0.3622 - val_accuracy: 0.8420\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2980 - accuracy: 0.8752 - val_loss: 0.3952 - val_accuracy: 0.8287\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2795 - accuracy: 0.8830 - val_loss: 0.3751 - val_accuracy: 0.8381\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2668 - accuracy: 0.8913 - val_loss: 0.3695 - val_accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2493 - accuracy: 0.8971 - val_loss: 0.3900 - val_accuracy: 0.8253\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2334 - accuracy: 0.9053 - val_loss: 0.3895 - val_accuracy: 0.8366\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc90fca3950>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRu2xHOIt_QE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}